{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_Mu6SQvZ8wA"
   },
   "source": [
    "# UBC CPSC 532V (2023W2) NLP COMMONSENSE\n",
    "\n",
    "## Assignment 2\n",
    "\n",
    "The goal of this assignment is to develop an LLM-based in-context learning model for a multiple-choice commonsense QA task and to test the contribution of adding external commonsense knowledge from a KB.\n",
    "\n",
    "## Group A2 4\n",
    "\n",
    "**Juntai Cao** (50171404); **Yilin Yang** (24754350); **Yuwei Yin** (36211928).\n",
    "\n",
    "(Authors contributed equally and listed alphabetically.)\n",
    "\n",
    "The code and report, as well as all results, are available on [GitHub](https://github.com/YuweiYin/UBC_CPSC_532V/tree/master/Assignment_2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "### Prerequisites \n",
    "\n",
    "Install Huggingface `transformers`, `datasets`, and `xformers` (for accelerating computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following pip commands for installing Python packages\n",
    "# !pip install datasets transformers torch tqdm numpy\n",
    "# !pip install keybert sentence-transformers openai\n",
    "# !pip install setuptools wheel spacy\n",
    "# !python -m spacy download en_core_web_md\n",
    "# !pip install xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "### Load COPA\n",
    "\n",
    "We will use the [Datasets](https://github.com/huggingface/datasets) library to download the data. This can be easily done with the function `load_dataset`. This function will cache the dataset to avoid downloading it again the next time you run this cell.\n",
    "\n",
    "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set. We will immediately convert them to lists. We will only use the train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tcowx0CDabSH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "copa = load_dataset(\"super_glue\", \"copa\")\n",
    "train_set = list(copa[\"train\"])\n",
    "val_set = list(copa[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtUQ9SbcaXGk"
   },
   "source": [
    "## Part 1: In-Context Learning\n",
    "\n",
    "We are using a prompting approach in which the model generates the predicted answer. Specifically, we follow the in-context / few-shot setup in which the model sees a few questions along with their answers (which are taken from the training set), followed by a single target question for which it generates the answer.\n",
    "\n",
    "The first step is to present a COPA example in natural language. This is done with the following `single_example_prompt` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GfxVugizb6YU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: My body cast a shadow over the grass. What could have caused this?\n",
      "1) The sun was rising.\n",
      "2) The grass was cut.\n",
      "A: 1\n"
     ]
    }
   ],
   "source": [
    "question_type_to_nl = {\"cause\": \"What could have caused this?\", \n",
    "                       \"effect\": \"What might have happened as a result?\"}\n",
    "\n",
    "def single_example_prompt(example, include_answer=False):\n",
    "    prompt = f\"Q: {example['premise']} {question_type_to_nl[example['question']]}\" + \\\n",
    "             f\"\\n1) {example['choice1']}\\n2) {example['choice2']}\"\n",
    "\n",
    "    if include_answer:\n",
    "      prompt += f\"\\nA: {example['label'] + 1}\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "print(single_example_prompt(train_set[0], include_answer=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYsHgEo5goXq"
   },
   "source": [
    "Then, we would like to create a prompt containing several in-context examples (5, in this case) followed by the target example. This is what `create_prompt` does. It will get the function to create a prompt for a single example as an argument so that we can re-use it when we change the single example prompt format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PSDd5acZdSbX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_IN_CONTEXT = 5\n",
    "\n",
    "# Randomly select in context examples from the train set\n",
    "random.seed(28)  # make sure use the required in_context_examples\n",
    "in_context_examples = random.sample(train_set, NUM_IN_CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The woman felt lonely. What might have happened as a result?\n",
      "1) She renovated her kitchen.\n",
      "2) She adopted a cat.\n",
      "A: 2\n",
      "\n",
      "Q: The mother needed help looking after her children. What might have happened as a result?\n",
      "1) She sent the children to daycare.\n",
      "2) She gave up custody of the children.\n",
      "A: 1\n",
      "\n",
      "Q: I learned how to play the board game. What could have caused this?\n",
      "1) My friend explained the rules to me.\n",
      "2) My friend got the rules wrong.\n",
      "A: 1\n",
      "\n",
      "Q: The woman's eyeglasses fogged up. What could have caused this?\n",
      "1) She reclined by the pool.\n",
      "2) She entered the sauna.\n",
      "A: 2\n",
      "\n",
      "Q: I ran out of breath. What could have caused this?\n",
      "1) I climbed several flights of stairs.\n",
      "2) I read several chapters of the book.\n",
      "A: 1\n",
      "\n",
      "Q: The man turned on the faucet. What might have happened as a result?\n",
      "1) The toilet filled with water.\n",
      "2) Water flowed from the spout.\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(in_context_examples, target, single_fn=single_example_prompt):\n",
    "    return \"\\n\\n\".join(\n",
    "        [single_fn(ex, include_answer=True) \n",
    "        for ex in in_context_examples]) + \"\\n\\n\" + single_fn(target)\n",
    "\n",
    "prompt = create_prompt(in_context_examples, val_set[0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from transformers import set_seed\n",
    "\n",
    "# Set all random seeds to guarantee consistent and reproducible results\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4qOsCtIi8fv"
   },
   "source": [
    "You can use [OPT-350M](https://huggingface.co/facebook/opt-350m), [GPT-Neo 125M](https://huggingface.co/EleutherAI/gpt-neo-125m) or whichever other LLM you'd like. Those two you should be able to run locally.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "czrzCP_MWeV5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "MODEL_NAME = \"EleutherAI/gpt-neo-125m\"\n",
    "# MODEL_NAME = \"facebook/opt-350m\"\n",
    "# MODEL_NAME = \"openai-community/gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "generator = pipeline(model=MODEL_NAME, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCCP8BE1jIkY"
   },
   "source": [
    "Let's generate an answer to this prompt (i.e. an answer for the first example in the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4oc4GlAOWj8A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 1\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of tokens to predict\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "expected_tokens = len(tokenizer([\"\\nA: 1\"])[0])\n",
    "gen_config = GenerationConfig(\n",
    "    min_new_tokens=expected_tokens, max_new_tokens=expected_tokens + 2, \n",
    "    do_sample=True, top_p=0.9, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Feed the prompt we created and generated up to `expected_tokens` tokens.\n",
    "output = generator(prompt, generation_config=gen_config)[0]['generated_text'][len(prompt):].strip()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipp4AczxjTvx"
   },
   "source": [
    "In particular, we need to determine whether the model predicted 1 or 2, which we do in `predict`. We will convert `1` to 0 and `2` to 1 to match the gold labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(generator, prompt, gen_config=None):\n",
    "    answer = generator(prompt, generation_config=gen_config)[0][\n",
    "        'generated_text'][len(prompt):].strip()\n",
    "    # print(answer)\n",
    "\n",
    "    # If the model generated another instance, remove it\n",
    "    if \"Q:\" in answer:\n",
    "        answer = answer[:answer.index(\"Q:\")]\n",
    "\n",
    "    # Find \"A: 1\" or \"A: 2\"\n",
    "    m = re.search(\"A:\\s*([1-2])\", answer)\n",
    "    if m is not None:\n",
    "        answer = int(m.group(1)) - 1\n",
    "    else:\n",
    "        answer = None\n",
    "    # print(answer)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def predict_retry(generator, prompt, gen_config=None):\n",
    "    \"\"\"\n",
    "    Retry generation if the answer format is not correct\n",
    "    (i.e., containing \"A: 1\" or \"A: 2\"), making sure that\n",
    "    there is no NULL when computing the accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    answer = None\n",
    "    retry_limit = 10\n",
    "    for retry_cnt in range(retry_limit):  # Retry if the output format is not correct\n",
    "        answer = generator(prompt, generation_config=gen_config)[0][\n",
    "            'generated_text'][len(prompt):].strip()\n",
    "        # print(answer)\n",
    "\n",
    "        # If the model generated another instance, remove it\n",
    "        if \"Q:\" in answer:\n",
    "            answer = answer[:answer.index(\"Q:\")]\n",
    "\n",
    "        # Find \"A: 1\" or \"A: 2\"\n",
    "        m = re.search(\"A:\\s*([1-2])\", answer)\n",
    "        if m is not None:\n",
    "            answer = int(m.group(1)) - 1\n",
    "            break\n",
    "\n",
    "    if answer is None:\n",
    "        # print(f\"Tried {retry_limit} times, but the answer is still None.\\nPROMPT: {prompt}\\n\")\n",
    "        print(f\"Tried {retry_limit} times, but the answer is still None.\")\n",
    "    # else:\n",
    "    #     print(f\"Final answer: {answer}\")\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = create_prompt(in_context_examples, val_set[0])\n",
    "predict_retry(generator, prompt, gen_config=gen_config)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYbzKVskkxGG"
   },
   "source": [
    "Finally, let's compute the predictions for the entire validation set \n",
    "and then compute the accuracy for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataset, predictions):\n",
    "    gold = np.array([ex[\"label\"] for ex in dataset])\n",
    "    preds = np.array(predictions)\n",
    "    accuracy = (preds == gold).astype(np.float32).mean().item() * 100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae89aae17d84459d9da940660c7a96a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.00%\n"
     ]
    }
   ],
   "source": [
    "val_predictions = [\n",
    "    predict_retry(\n",
    "        generator,\n",
    "        create_prompt(in_context_examples, ex),\n",
    "        gen_config=gen_config\n",
    "    ) for ex in tqdm.notebook.tqdm(val_set)\n",
    "]\n",
    "\n",
    "print(f\"Accuracy: {compute_accuracy(val_set, val_predictions):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChHa-0zm2y0A"
   },
   "source": [
    "Let's save the predictions to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RrOzTlqP207p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to output/basic_predictions-gpt-neo-125m.jsonl\n"
     ]
    }
   ],
   "source": [
    "res_fp_1 = \"output/basic_predictions-{}.jsonl\".format(MODEL_NAME.split(\"/\")[-1])\n",
    "print(f\"Saving to {res_fp_1}\")\n",
    "\n",
    "with open(res_fp_1, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for ex, pred in zip(val_set, val_predictions):\n",
    "        new_ex = ex.copy()\n",
    "        new_ex[\"prediction\"] = pred\n",
    "        f_out.write(json.dumps(new_ex) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuray is 0.51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'premise': 'The detective revealed an anomaly in the case.',\n",
       "  'choice1': 'He finalized his theory.',\n",
       "  'choice2': 'He scrapped his theory.',\n",
       "  'question': 'effect',\n",
       "  'idx': 52,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'The boy skipped dinner.',\n",
       "  'choice1': 'His mother cooked his favorite meal.',\n",
       "  'choice2': 'He ate a big lunch.',\n",
       "  'question': 'cause',\n",
       "  'idx': 55,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'My eyes became red and puffy.',\n",
       "  'choice1': 'I was sobbing.',\n",
       "  'choice2': 'I was laughing.',\n",
       "  'question': 'cause',\n",
       "  'idx': 7,\n",
       "  'label': 0,\n",
       "  'prediction': 1},\n",
       " {'premise': 'The bride got cold feet before the wedding.',\n",
       "  'choice1': 'The wedding guests brought gifts.',\n",
       "  'choice2': 'She called the wedding off.',\n",
       "  'question': 'effect',\n",
       "  'idx': 38,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'The teacher assigned homework to the students.',\n",
       "  'choice1': 'The students passed notes.',\n",
       "  'choice2': 'The students groaned.',\n",
       "  'question': 'effect',\n",
       "  'idx': 72,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'The tree branch landed in the river.',\n",
       "  'choice1': 'The branch moved downstream.',\n",
       "  'choice2': \"The river's current became stronger.\",\n",
       "  'question': 'effect',\n",
       "  'idx': 71,\n",
       "  'label': 0,\n",
       "  'prediction': 1},\n",
       " {'premise': 'The child learned how to read.',\n",
       "  'choice1': 'He began attending school.',\n",
       "  'choice2': 'He skipped a grade in school.',\n",
       "  'question': 'cause',\n",
       "  'idx': 54,\n",
       "  'label': 0,\n",
       "  'prediction': 1},\n",
       " {'premise': 'The student was in a rush to get to school on time.',\n",
       "  'choice1': 'He left his assignment at home.',\n",
       "  'choice2': 'He brought his lunch to school.',\n",
       "  'question': 'effect',\n",
       "  'idx': 43,\n",
       "  'label': 0,\n",
       "  'prediction': 1},\n",
       " {'premise': 'The dog barked.',\n",
       "  'choice1': 'The cat lounged on the couch.',\n",
       "  'choice2': 'A knock sounded at the door.',\n",
       "  'question': 'cause',\n",
       "  'idx': 67,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'I pulled the rubber band.',\n",
       "  'choice1': 'It flung across the room.',\n",
       "  'choice2': 'It stretched.',\n",
       "  'question': 'effect',\n",
       "  'idx': 48,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'I stayed up late.',\n",
       "  'choice1': 'I had vivid dreams that night.',\n",
       "  'choice2': 'I was tired in the morning.',\n",
       "  'question': 'effect',\n",
       "  'idx': 78,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'The secretary put the caller on hold.',\n",
       "  'choice1': \"The caller's phone lost reception.\",\n",
       "  'choice2': 'The caller waited on the line.',\n",
       "  'question': 'effect',\n",
       "  'idx': 30,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'The man received a parking ticket.',\n",
       "  'choice1': 'He parallel parked on the street.',\n",
       "  'choice2': 'The parking meter expired.',\n",
       "  'question': 'cause',\n",
       "  'idx': 92,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'I regained composure from my fit of anger.',\n",
       "  'choice1': 'My heart pounded.',\n",
       "  'choice2': 'I took deep breaths.',\n",
       "  'question': 'cause',\n",
       "  'idx': 21,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'The pair of students came under scrutiny by the teacher.',\n",
       "  'choice1': 'The students both received excellent grades.',\n",
       "  'choice2': 'Their responses on the assignment were identical.',\n",
       "  'question': 'cause',\n",
       "  'idx': 42,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'I pushed the wagon.',\n",
       "  'choice1': 'The objects in the wagon fell out.',\n",
       "  'choice2': 'The wagon wheels spun forward.',\n",
       "  'question': 'effect',\n",
       "  'idx': 75,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'The woman banished the children from her property.',\n",
       "  'choice1': 'The children hit a ball into her yard.',\n",
       "  'choice2': 'The children trampled through her garden.',\n",
       "  'question': 'cause',\n",
       "  'idx': 16,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'My ears were ringing.',\n",
       "  'choice1': 'I went to a museum.',\n",
       "  'choice2': 'I went to a concert.',\n",
       "  'question': 'cause',\n",
       "  'idx': 95,\n",
       "  'label': 1,\n",
       "  'prediction': 0},\n",
       " {'premise': 'The student forgot to do her assignment.',\n",
       "  'choice1': 'She made up an excuse to tell the teacher.',\n",
       "  'choice2': 'The teacher promoted her to the next grade.',\n",
       "  'question': 'effect',\n",
       "  'idx': 66,\n",
       "  'label': 0,\n",
       "  'prediction': 1},\n",
       " {'premise': 'The friends decided to share the hamburger.',\n",
       "  'choice1': 'They cut the hamburger in half.',\n",
       "  'choice2': 'They ordered fries with the hamburger.',\n",
       "  'question': 'effect',\n",
       "  'idx': 40,\n",
       "  'label': 0,\n",
       "  'prediction': 1}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts = []\n",
    "with open(res_fp_1, \"r\", encoding=\"utf-8\") as f_in:\n",
    "    for line in f_in:\n",
    "        predicts.append(json.loads(line))\n",
    "\n",
    "incorrect_predicts = []\n",
    "for predict in predicts:\n",
    "    if predict[\"label\"] != predict[\"prediction\"]:\n",
    "        incorrect_predicts.append(predict)\n",
    "\n",
    "accuracy = 1 - len(incorrect_predicts) / len(predicts)\n",
    "print(f\"The prediction accuray is {accuracy:.2f}\")\n",
    "\n",
    "selected_incorrect_predicts = random.sample(incorrect_predicts, 20)\n",
    "selected_incorrect_predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilNjMW6Lvz6H"
   },
   "source": [
    "## Part 2: Chain-of-Thought Prompting\n",
    "\n",
    "In this part, you will add a reasoning step to the examples, following [Wei et al. (2022)](https://arxiv.org/abs/2201.11903). Take a look at the paper the get an idea of a good rationale / reasoning chain. Note that we are still following the in-context / few-shot setup, so you will need to manually come up with a rationale for each of the in-context examples.\n",
    "\n",
    "First, let us create a new function `single_example_prompt_with_cot` that adds the rationale to the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EjOAe8XxwoGb"
   },
   "outputs": [],
   "source": [
    "def single_example_prompt_with_cot(example, include_answer=False):\n",
    "    prompt = f\"Q: {example['premise']} {question_type_to_nl[example['question']]}\" + \\\n",
    "             f\"\\n1) {example['choice1']}\\n2) {example['choice2']}\"\n",
    "\n",
    "    if include_answer:\n",
    "      prompt += f\"\\nRationale: {example['rationale']}\"\n",
    "      prompt += f\"\\nA: {example['label'] + 1}\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1WJApWcqtGX"
   },
   "source": [
    "We will also update the `predict` function to return both the answer and the generated rationale, which you can use to analyze the errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_cot(generator, prompt, gen_config=None):\n",
    "    answer = generator(prompt, generation_config=gen_config)[0][\n",
    "        \"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "    # If the model generated another instance, remove it\n",
    "    if \"Q:\" in answer:\n",
    "        answer = answer[:answer.index(\"Q:\")]\n",
    "\n",
    "    # If the model generated a rationale\n",
    "    rationale = None\n",
    "    m = re.search(\"Rationale:\\s?([^\\n]+)\", answer)\n",
    "    if m is not None:\n",
    "        rationale = m.group(1)\n",
    "\n",
    "    # Find \"A: 1\" or \"A: 2\"\n",
    "    pred = None\n",
    "    m = re.search(\"A:\\s?([1-2])\", answer)\n",
    "    if m is not None:\n",
    "        pred = int(m.group(1)) - 1\n",
    "\n",
    "    return pred, rationale\n",
    "\n",
    "\n",
    "def predict_with_cot_retry(generator, prompt, gen_config=None):\n",
    "    \"\"\"\n",
    "    Retry generation if the answer format is not correct\n",
    "    (i.e., containing \"A: 1\" or \"A: 2\"), making sure that\n",
    "    there is no NULL when computing the accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    pred = None\n",
    "    retry_limit = 10\n",
    "    for retry_cnt in range(retry_limit):  # Retry if the output format is not correct\n",
    "        answer = generator(prompt, generation_config=gen_config)[0][\n",
    "            \"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "        # If the model generated another instance, remove it\n",
    "        if \"Q:\" in answer:\n",
    "            answer = answer[:answer.index(\"Q:\")]\n",
    "\n",
    "        # If the model generated a rationale\n",
    "        rationale = None\n",
    "        m = re.search(\"Rationale:\\s?([^\\n]+)\", answer)\n",
    "        if m is not None:\n",
    "            rationale = m.group(1)\n",
    "\n",
    "        # Find \"A: 1\" or \"A: 2\"\n",
    "        pred = None\n",
    "        m = re.search(\"A:\\s?([1-2])\", answer)\n",
    "        if m is not None:\n",
    "            pred = int(m.group(1)) - 1\n",
    "            break\n",
    "\n",
    "    if pred is None:\n",
    "        print(f\"Tried {retry_limit} times, but the prediction is still None.\\nPROMPT: {prompt}\\n\")\n",
    "    # else:\n",
    "    #     print(f\"Final prediction: {pred}\")\n",
    "\n",
    "    return pred, rationale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E8HAovow7Gh"
   },
   "source": [
    "Now, we need to add a `rationale` field for the in-context examples. Let's print the examples, and in the next cell, you will add the rationales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5ACd0NIUxOTD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The woman felt lonely. What might have happened as a result?\n",
      "1) She renovated her kitchen.\n",
      "2) She adopted a cat.\n",
      "\n",
      "Q: The mother needed help looking after her children. What might have happened as a result?\n",
      "1) She sent the children to daycare.\n",
      "2) She gave up custody of the children.\n",
      "\n",
      "Q: I learned how to play the board game. What could have caused this?\n",
      "1) My friend explained the rules to me.\n",
      "2) My friend got the rules wrong.\n",
      "\n",
      "Q: The woman's eyeglasses fogged up. What could have caused this?\n",
      "1) She reclined by the pool.\n",
      "2) She entered the sauna.\n",
      "\n",
      "Q: I ran out of breath. What could have caused this?\n",
      "1) I climbed several flights of stairs.\n",
      "2) I read several chapters of the book.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ex in in_context_examples:\n",
    "    print(single_example_prompt(ex) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zT3qGiGBxhKP"
   },
   "outputs": [],
   "source": [
    "in_context_examples_cot = in_context_examples.copy()\n",
    "\n",
    "############################################\n",
    "#  Complete the following code\n",
    "############################################\n",
    "in_context_examples_cot[0][\"rationale\"] = \"The answer is 2 bacause: the woman adopted a cat to alleviate her feelings of loneliness, seeking companionship and emotional support.\"\n",
    "in_context_examples_cot[1][\"rationale\"] = \"The answer is 1 bacause: The mother sent her children to daycare to receive assistance with childcare responsibilities, enabling her to fulfill other obligations or work commitments.\"\n",
    "in_context_examples_cot[2][\"rationale\"] = \"The answer is 1 bacause: My friend's explanation of the rules facilitated my learning of the board game, providing clarity and guidance on gameplay mechanics.\"\n",
    "in_context_examples_cot[3][\"rationale\"] = \"The answer is 2 bacause: As she entered the sauna, the temperature change caused condensation on the woman's eyeglasses, resulting in fogging up due to the heat and moisture.\"\n",
    "in_context_examples_cot[4][\"rationale\"] = \"The answer is 1 bacause: Climbing several flights of stairs increased my physical exertion, leading to a rapid depletion of oxygen and causing me to run out of breath.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb1Nz8zhzIYw"
   },
   "source": [
    "Let's look at an example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jw_sQI7fzQb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The woman felt lonely. What might have happened as a result?\n",
      "1) She renovated her kitchen.\n",
      "2) She adopted a cat.\n",
      "Rationale: The answer is 2 bacause: the woman adopted a cat to alleviate her feelings of loneliness, seeking companionship and emotional support.\n",
      "A: 2\n",
      "\n",
      "Q: The mother needed help looking after her children. What might have happened as a result?\n",
      "1) She sent the children to daycare.\n",
      "2) She gave up custody of the children.\n",
      "Rationale: The answer is 1 bacause: The mother sent her children to daycare to receive assistance with childcare responsibilities, enabling her to fulfill other obligations or work commitments.\n",
      "A: 1\n",
      "\n",
      "Q: I learned how to play the board game. What could have caused this?\n",
      "1) My friend explained the rules to me.\n",
      "2) My friend got the rules wrong.\n",
      "Rationale: The answer is 1 bacause: My friend's explanation of the rules facilitated my learning of the board game, providing clarity and guidance on gameplay mechanics.\n",
      "A: 1\n",
      "\n",
      "Q: The woman's eyeglasses fogged up. What could have caused this?\n",
      "1) She reclined by the pool.\n",
      "2) She entered the sauna.\n",
      "Rationale: The answer is 2 bacause: As she entered the sauna, the temperature change caused condensation on the woman's eyeglasses, resulting in fogging up due to the heat and moisture.\n",
      "A: 2\n",
      "\n",
      "Q: I ran out of breath. What could have caused this?\n",
      "1) I climbed several flights of stairs.\n",
      "2) I read several chapters of the book.\n",
      "Rationale: The answer is 1 bacause: Climbing several flights of stairs increased my physical exertion, leading to a rapid depletion of oxygen and causing me to run out of breath.\n",
      "A: 1\n",
      "\n",
      "Q: The man turned on the faucet. What might have happened as a result?\n",
      "1) The toilet filled with water.\n",
      "2) Water flowed from the spout.\n"
     ]
    }
   ],
   "source": [
    "prompt = create_prompt(in_context_examples_cot, val_set[0], single_fn=single_example_prompt_with_cot)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAO9sfaBzzF8"
   },
   "source": [
    "Let's see what the LLM generates for the target answer. We will now allow for more tokens to be generated, since the model is also expected to generate the rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output length according the the rationales in the in context examples.\n",
    "shortest_rationale = in_context_examples_cot[\n",
    "    np.argmin([len(ex[\"rationale\"].split()) for ex in in_context_examples_cot])\n",
    "    ][\"rationale\"]\n",
    "\n",
    "longest_rationale = in_context_examples_cot[\n",
    "    np.argmax([len(ex[\"rationale\"].split()) for ex in in_context_examples_cot])\n",
    "    ][\"rationale\"]\n",
    "\n",
    "min_tokens = len(tokenizer([f\"\\nRationale: {shortest_rationale} \\nA: 1\"])[0])\n",
    "max_tokens = len(tokenizer([f\"\\nRationale: {longest_rationale} \\nA: 1\"])[0])\n",
    "\n",
    "# Update the generation config accordingly\n",
    "cot_gen_config = GenerationConfig(min_new_tokens=min_tokens, \n",
    "                                  max_new_tokens=max_tokens, \n",
    "                                  do_sample=True, top_p=0.9, \n",
    "                                  eos_token_id=tokenizer.eos_token_id, \n",
    "                                  pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZjczD1r8zzaS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The man turned on the faucet. What might have happened as a result?\n",
      "1) The toilet filled with water.\n",
      "2) Water flowed from the spout.\n",
      "Rationale: The answer is 1 bacause: The man turned on the faucet.\n",
      "A: 0\n"
     ]
    }
   ],
   "source": [
    "answer, rationale = predict_with_cot_retry(\n",
    "    generator, create_prompt(\n",
    "        in_context_examples_cot, val_set[0],\n",
    "        single_fn=single_example_prompt_with_cot\n",
    "    ),\n",
    "    gen_config=cot_gen_config\n",
    ")\n",
    "\n",
    "print(single_example_prompt(val_set[0]))\n",
    "print(f\"Rationale: {rationale}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXDhpcIDzt07"
   },
   "source": [
    "Finally, let's predict the entire validation set, compute the accuracy, and save the predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HtjkWQV0z0BO"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae1040cf09b43b3baffe0761beccbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.00%\n"
     ]
    }
   ],
   "source": [
    "val_predictions_cot, val_rationales = zip(*\n",
    "    [predict_with_cot_retry(\n",
    "        generator,\n",
    "        create_prompt(\n",
    "            in_context_examples_cot,\n",
    "            ex,\n",
    "            single_fn=single_example_prompt_with_cot),\n",
    "        gen_config=cot_gen_config) \n",
    "    for ex in tqdm.notebook.tqdm(val_set)\n",
    "    ])\n",
    "\n",
    "print(f\"Accuracy: {compute_accuracy(val_set, val_predictions_cot):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to output/cot_predictions-gpt-neo-125m.jsonl\n"
     ]
    }
   ],
   "source": [
    "res_fp_2 = \"output/cot_predictions-{}.jsonl\".format(MODEL_NAME.split(\"/\")[-1])\n",
    "print(f\"Saving to {res_fp_2}\")\n",
    "\n",
    "with open(res_fp_2, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for ex, pred, rationale in zip(val_set, val_predictions_cot, val_rationales):\n",
    "        new_ex = ex.copy()\n",
    "        new_ex[\"prediction\"] = pred\n",
    "        new_ex[\"rationale\"] = rationale\n",
    "        f_out.write(json.dumps(new_ex) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuray is 0.58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'premise': 'My ears were ringing.',\n",
       "  'choice1': 'I went to a museum.',\n",
       "  'choice2': 'I went to a concert.',\n",
       "  'question': 'cause',\n",
       "  'idx': 95,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: The museum suggested to me that I learn how to play the board game.'},\n",
       " {'premise': 'The driver got a flat tire.',\n",
       "  'choice1': 'He went over the speed limit.',\n",
       "  'choice2': 'He ran over a nail.',\n",
       "  'question': 'cause',\n",
       "  'idx': 25,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: Driving a car reduced my strength and strength training, allowing me to work on my running ability.'},\n",
       " {'premise': 'The pair of students came under scrutiny by the teacher.',\n",
       "  'choice1': 'The students both received excellent grades.',\n",
       "  'choice2': 'Their responses on the assignment were identical.',\n",
       "  'question': 'cause',\n",
       "  'idx': 42,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': \"The answer is 1 bacause: The students' responses on the assignment were identical.\"},\n",
       " {'premise': 'The woman resigned from her job.',\n",
       "  'choice1': 'She aspired to hold an executive position in the firm.',\n",
       "  'choice2': 'She believed her superiors were acting unethically.',\n",
       "  'question': 'cause',\n",
       "  'idx': 13,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: The woman resigned from her job.'},\n",
       " {'premise': \"The community learned of the man's death.\",\n",
       "  'choice1': 'His family buried him in the cemetery.',\n",
       "  'choice2': 'His obituary appeared in the newspaper.',\n",
       "  'question': 'cause',\n",
       "  'idx': 11,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: The obituary appears in the newspaper.'},\n",
       " {'premise': \"The woman's hair fell in her face.\",\n",
       "  'choice1': 'She pulled her hair back with a clip.',\n",
       "  'choice2': 'She lathered shampoo into her hair.',\n",
       "  'question': 'effect',\n",
       "  'idx': 46,\n",
       "  'label': 0,\n",
       "  'prediction': 1,\n",
       "  'rationale': 'The answer is 2 bacause: The woman pulled her hair back with a clip.'},\n",
       " {'premise': 'The seasons changed from summer to autumn.',\n",
       "  'choice1': 'People evacuated their homes.',\n",
       "  'choice2': 'Leaves fell from the trees.',\n",
       "  'question': 'effect',\n",
       "  'idx': 73,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: The leaves of the trees decreased the temperatures to the point that they were becoming colder, resulting in a slight increase in the number of insects.'},\n",
       " {'premise': 'The woman was in a bad mood.',\n",
       "  'choice1': 'She engaged in small talk with her friend.',\n",
       "  'choice2': 'She told her friend to leave her alone.',\n",
       "  'question': 'effect',\n",
       "  'idx': 99,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: her words distracted my understanding and made my life unpleasant.'},\n",
       " {'premise': 'The detective revealed an anomaly in the case.',\n",
       "  'choice1': 'He finalized his theory.',\n",
       "  'choice2': 'He scrapped his theory.',\n",
       "  'question': 'effect',\n",
       "  'idx': 52,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': \"The answer is 1 bacause: The detective's description of the anomaly resulted in me not completing my task to the highest-level of my game.\"},\n",
       " {'premise': 'The dog barked.',\n",
       "  'choice1': 'The cat lounged on the couch.',\n",
       "  'choice2': 'A knock sounded at the door.',\n",
       "  'question': 'cause',\n",
       "  'idx': 67,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: The cat was knocking on the door, trying to enter the room.'},\n",
       " {'premise': 'The man defied the authorities of the church.',\n",
       "  'choice1': 'He donated money to the church.',\n",
       "  'choice2': 'He was excommunicated from the church.',\n",
       "  'question': 'effect',\n",
       "  'idx': 45,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: My excommunicated friend.'},\n",
       " {'premise': 'I tidied up my house.',\n",
       "  'choice1': 'I was swamped with work.',\n",
       "  'choice2': 'I was expecting company.',\n",
       "  'question': 'cause',\n",
       "  'idx': 96,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: I tried the door after I got there to open it. It had not been opened.'},\n",
       " {'premise': 'I pushed the wagon.',\n",
       "  'choice1': 'The objects in the wagon fell out.',\n",
       "  'choice2': 'The wagon wheels spun forward.',\n",
       "  'question': 'effect',\n",
       "  'idx': 75,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: When the objects fell out, they lost their power to drive the wagon back into the ground, causing the wagon to crash.'},\n",
       " {'premise': 'Political violence broke out in the nation.',\n",
       "  'choice1': 'Many citizens relocated to the capitol.',\n",
       "  'choice2': 'Many citizens took refuge in other territories.',\n",
       "  'question': 'effect',\n",
       "  'idx': 83,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: People who moved in by virtue of their allegiance to the ruling government were expelled from the country.'},\n",
       " {'premise': 'The bride got cold feet before the wedding.',\n",
       "  'choice1': 'The wedding guests brought gifts.',\n",
       "  'choice2': 'She called the wedding off.',\n",
       "  'question': 'effect',\n",
       "  'idx': 38,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: The bride called the wedding off.'},\n",
       " {'premise': 'The woman walked with crutches.',\n",
       "  'choice1': 'She shaved her legs.',\n",
       "  'choice2': 'She broke her leg.',\n",
       "  'question': 'cause',\n",
       "  'idx': 31,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': \"The answer is 1 bacause: The woman's legs were not fixed.\"},\n",
       " {'premise': 'The secretary put the caller on hold.',\n",
       "  'choice1': \"The caller's phone lost reception.\",\n",
       "  'choice2': 'The caller waited on the line.',\n",
       "  'question': 'effect',\n",
       "  'idx': 30,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': \"The answer is 1 bacause: the callers' phone was disconnected.\"},\n",
       " {'premise': 'The couple travelled south for the winter.',\n",
       "  'choice1': 'They were retired.',\n",
       "  'choice2': 'They were separated.',\n",
       "  'question': 'cause',\n",
       "  'idx': 36,\n",
       "  'label': 0,\n",
       "  'prediction': 1,\n",
       "  'rationale': 'The answer is 2 bacause: When they were separated, they had difficulty maintaining their relationship.'},\n",
       " {'premise': \"The cook's eyes watered.\",\n",
       "  'choice1': 'He ran out of onions.',\n",
       "  'choice2': 'He cut an onion.',\n",
       "  'question': 'cause',\n",
       "  'idx': 18,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': \"The answer is 1 bacause: The cook's response to the answer was 2. He cut onions.\"},\n",
       " {'premise': 'I wanted to conserve energy.',\n",
       "  'choice1': 'I swept the floor in the unoccupied room.',\n",
       "  'choice2': 'I shut off the light in the unoccupied room.',\n",
       "  'question': 'effect',\n",
       "  'idx': 3,\n",
       "  'label': 1,\n",
       "  'prediction': 0,\n",
       "  'rationale': 'The answer is 1 bacause: The water in the swimming pool is heating up.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts = []\n",
    "with open(res_fp_2, \"r\", encoding=\"utf-8\") as f_in:\n",
    "    for line in f_in:\n",
    "        predicts.append(json.loads(line))\n",
    "\n",
    "incorrect_predicts = []\n",
    "for predict in predicts:\n",
    "    if predict[\"label\"] != predict[\"prediction\"]:\n",
    "        incorrect_predicts.append(predict)\n",
    "\n",
    "accuracy = 1 - len(incorrect_predicts) / len(predicts)\n",
    "print(f\"The prediction accuray is {accuracy:.2f}\")\n",
    "\n",
    "selected_incorrect_predicts = random.sample(incorrect_predicts, 20)\n",
    "selected_incorrect_predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inml-pRipYYs"
   },
   "source": [
    "## Part 3: In-Context Learning with External Knowledge\n",
    "\n",
    "In the last part, instead of letting the LLM generate its own rationales based on the manually-curated rationales for the in-context examples, we will attach a \"rationale\" from ConceptNet to each example (in-context and target). The first step will be to implement the `retrieve_knowledge` function, that gets a COPA instance and returns a rationale from ConceptNet. Base on your code from assignment 1 and choose one path to include in a natural language representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class ConceptNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.prefix_url = \"https://api.conceptnet.io\"\n",
    "        self.prefix_cid = \"/c/en/\"  # the prefix of every concept_id\n",
    "\n",
    "    def get_concept_id(self, word: str) -> str:\n",
    "        return self.prefix_cid + word.strip()\n",
    "\n",
    "    def get_url(self, cid: str) -> str:\n",
    "        return self.prefix_url + cid.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_concept(url: str, verbose: bool = False) -> dict:\n",
    "        try:\n",
    "            response = requests.get(url.strip()).json()\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\">>> >>> get_concept Exception: {e}\")\n",
    "            response = dict()\n",
    "        return response\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next_node(cid: str, edge: dict) -> Tuple[str, float, str, int]:\n",
    "        if edge[\"end\"][\"@id\"] != cid:\n",
    "            next_id, direction = edge[\"end\"][\"@id\"], 1\n",
    "        else:\n",
    "            next_id, direction = edge[\"start\"][\"@id\"], 0\n",
    "        return next_id, edge[\"weight\"], edge[\"rel\"][\"label\"], direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "class TextParser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.kw_model = KeyBERT()\n",
    "        self.st_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    def get_keywords_keybert(self, text: str, n_bag: int = 1) -> List[str]:\n",
    "        keywords = self.kw_model.extract_keywords(text, keyphrase_ngram_range=(1, n_bag), stop_words=\"english\")\n",
    "        keywords = [w[0].strip() for w in keywords]\n",
    "        return keywords\n",
    "\n",
    "    def keyword_sort(self, references: List[str], keywords: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        sort keywords by computing the average similarity between the current kw and each ref in references\n",
    "        \"\"\"\n",
    "        if len(keywords) <= 1:\n",
    "            return keywords\n",
    "        assert len(references) >= 1, f\"Assertion error: len(references) is {len(references)}, NOT >= 1\"\n",
    "\n",
    "        ref_emb_list = [self.st_model.encode(w, convert_to_tensor=True) for w in references]\n",
    "        kw_emb_list = [self.st_model.encode(w, convert_to_tensor=True) for w in keywords]\n",
    "\n",
    "        kw_simi_list = []\n",
    "        for idx, kw_emb in enumerate(kw_emb_list):\n",
    "            kw_word = keywords[idx]\n",
    "            simi_list = [float(util.pytorch_cos_sim(kw_emb, ref_emb)) for ref_emb in ref_emb_list]\n",
    "            assert len(simi_list) >= 1\n",
    "            avg_simi = float(sum(simi_list) / len(simi_list))\n",
    "            kw_simi_list.append((kw_word, avg_simi))\n",
    "\n",
    "        kw_simi_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        res_list = [kw_simi[0] for kw_simi in kw_simi_list]\n",
    "\n",
    "        return res_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "REL_TO_TEMPLATE = {\n",
    "    \"relatedto\": \"[w1] is like [w2]\",\n",
    "    \"externalurl\": \"[w1] is described at the following URL [w2]\",\n",
    "    \"formof\": \"[w1] is a form of the word [w2]\",\n",
    "    \"isa\": \"[w1] is a type of [w2]\",\n",
    "    \"notisa\": \"[w1] is not [w2]\",\n",
    "    \"partof\": \"[w1] is part of [w2]\",\n",
    "    \"usedfor\": \"[w1] is used for [w2]\",\n",
    "    \"capableof\": \"[w1] can [w2]\",\n",
    "    \"atlocation\": \"You are likely to find [w1] in [w2]\",\n",
    "    \"causes\": \"Sometimes [w1] causes [w2]\",\n",
    "    \"hasa\": \"[w1] has [w2]\",\n",
    "    \"hassubevent\": \"Something you do when you [w1] is [w2]\",\n",
    "    \"hasfirstsubevent\": \"the first thing you do when you [w1] is [w2]\",\n",
    "    \"haslastsubevent\": \"the last thing you do when you [w1] is [w2]\",\n",
    "    \"hasprerequisite\": \"In order for [w1] to happen, [w2] needs to happen\",\n",
    "    \"hasproperty\": \"[w1] is [w2]\",\n",
    "    \"hascontext\": \"[w1] is a word used in the context of [w2]\",\n",
    "    \"motivatedbygoal\": \"You would [w1] because you want to [w2]\",\n",
    "    \"obstructedby\": \"[w1] can be prevented by [w2]\",\n",
    "    \"desires\": \"[w1] wants [w2]\",\n",
    "    \"createdby\": \"[w1] is created by [w2]\",\n",
    "    \"synonym\": \"[w1] and [w2] have similar meanings\",\n",
    "    \"antonym\": \"[w1] is the opposite of [w2]\",\n",
    "    \"distinctfrom\": \"it cannot be both [w1] and [w2]\",\n",
    "    \"derivedfrom\": \"the word [w1] is derived from the word [w2]\",\n",
    "    \"definedas\": \"[w1] is defined as [w2]\",\n",
    "    \"entails\": \"if [w1] is happening, [w2] is also happening\",\n",
    "    \"mannerof\": \"[w1] is a specific way of doing [w2]\",\n",
    "    \"locatednear\": \"[w1] is located near [w2]\",\n",
    "    \"dbpedia\": \"[w1] is conceptually related to [w2]\",\n",
    "    \"similarto\": \"[w1] is similar to [w2]\",\n",
    "    \"etymologicallyrelatedto\": \"the word [w1] and the word [w2] have the same origin\",\n",
    "    \"etymologicallyderivedfrom\": \"the word [w1] comes from the word [w2]\",\n",
    "    \"causesdesire\": \"[w1] makes people want [w2]\",\n",
    "    \"madeof\": \"[w1] is made of [w2]\",\n",
    "    \"receivesaction\": \"[w1] can be [w2]\",\n",
    "    \"instanceof\": \"[w1] is an example of [w2]\",\n",
    "    \"notdesires\": \"[w1] does not want [w2]\",\n",
    "    \"notusedfor\": \"[w1] is not used for [w2]\",\n",
    "    \"notcapableof\": \"[w1] is not capable of [w2]\",\n",
    "    \"nothasproperty\": \"[w1] does not have the property of [w2]\",\n",
    "    \"notmadeof\": \"[w1] is not made of [w2]\"\n",
    "}\n",
    "\n",
    "\n",
    "class TextConverter:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.node_list, self.w_list, self.r_list = path\n",
    "\n",
    "    def convert(self, verbose: bool = False) -> dict:\n",
    "        node_relation_list = []\n",
    "        prompt_list = []\n",
    "        description_list = []\n",
    "        assert len(self.node_list) == len(self.w_list) + 1 == len(self.r_list) + 1\n",
    "\n",
    "        for i in range(len(self.r_list)):\n",
    "            if self.r_list[i][1] == 0:  # forward\n",
    "                src_rel_tgt = self.node_list[i], self.r_list[i][0], self.node_list[i + 1]\n",
    "            else:  # backward\n",
    "                src_rel_tgt = self.node_list[i + 1], self.r_list[i][0], self.node_list[i]\n",
    "\n",
    "            start_word = self.extract_word(src_rel_tgt[0])\n",
    "            rel = src_rel_tgt[1]\n",
    "            end_word = self.extract_word(src_rel_tgt[2])\n",
    "\n",
    "            prompt = \"\"\n",
    "            cur_description = REL_TO_TEMPLATE[rel.lower()].replace(\"[w1]\", start_word).replace(\"[w2]\", end_word)\n",
    "\n",
    "            if verbose:\n",
    "                print(cur_description)\n",
    "\n",
    "            node_relation_list.append(src_rel_tgt)\n",
    "            prompt_list.append(prompt)\n",
    "            description_list.append(cur_description)\n",
    "\n",
    "        start_node = self.extract_word(self.node_list[0])\n",
    "        end_node = self.extract_word(self.node_list[-1])\n",
    "\n",
    "        data_dict = {\n",
    "            \"start_node\": start_node,\n",
    "            \"end_node\": end_node,\n",
    "            \"node_list\": self.node_list,\n",
    "            \"w_list\": self.w_list,\n",
    "            \"r_list\": self.r_list,\n",
    "            \"node_relation_list\": node_relation_list,\n",
    "            \"prompt_list\": prompt_list,\n",
    "            \"description_list\": description_list,\n",
    "        }\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_word(node: str) -> str:\n",
    "        node_words = node.split(\"/\")\n",
    "        concept_word = node_words[3].replace(\"_\", \" \").strip()\n",
    "        return concept_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Tuple, List, Set\n",
    "\n",
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "class DijkstraSearchBiSource:\n",
    "\n",
    "    def __init__(self):\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "        self.nlp = nlp\n",
    "        self.kw_model = KeyBERT()\n",
    "        self.st_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.src_word = \"\"\n",
    "        self.tgt_word = \"\"\n",
    "        self.src_emb = None\n",
    "        self.tgt_emb = None\n",
    "        self.simi_threshold = 0.3  # if the similarity between the next node and target one is low, ignore this node\n",
    "\n",
    "    def word_similarity(self, word_1: str, word_2: str) -> float:\n",
    "        string = f\"{word_1.strip()} {word_2.strip()}\"\n",
    "        tokens = self.nlp(string)\n",
    "        similarity = tokens[0].similarity(tokens[1])\n",
    "        return similarity\n",
    "\n",
    "    def dijkstra_path_search(self, src_word: str, tgt_word: str, max_depth: int = 10, verbose: bool = False):\n",
    "        src_word = src_word.strip()\n",
    "        tgt_word = tgt_word.strip()\n",
    "        self.src_word = src_word\n",
    "        self.tgt_word = tgt_word\n",
    "        self.src_emb = self.st_model.encode(src_word, convert_to_tensor=True)\n",
    "        self.tgt_emb = self.st_model.encode(tgt_word, convert_to_tensor=True)\n",
    "        src_tgt_similarity_sent = float(util.pytorch_cos_sim(self.src_emb, self.tgt_emb))\n",
    "        if verbose:\n",
    "            print(f\">>> dijkstra_path_search (bi-source weighted BFS; max_depth = {max_depth}): \"\n",
    "                  f\"From \\\"{src_word}\\\" To \\\"{tgt_word}\\\" (Similarity: {src_tgt_similarity_sent:.3f})\")\n",
    "\n",
    "        # self.simi_threshold = src_tgt_similarity_sent / 2\n",
    "        self.simi_threshold = src_tgt_similarity_sent * 2 / 3\n",
    "\n",
    "        bfs_src = [[], []]  # the list of concept ids (source nodes of the current BFS)\n",
    "        bfs_tgt = [[], []]  # the list of concept ids (target nodes of the current BFS)\n",
    "        visited = [set(), set()]  # avoid looping\n",
    "        weights = [dict(), dict()]  # edge weight in Dij alg\n",
    "        prev_node = [dict(), dict()]  # to backtrace: prev_node[next_node] = cur_node\n",
    "        relations = [dict(), dict()]  # to backtrace: relations[next_node] = [relation, direction]\n",
    "        matches = list()  # matched nodes (~= tgt_word)\n",
    "        matches_set = set() # set(matches)\n",
    "\n",
    "        continue_bfs_flag = True\n",
    "\n",
    "        conceptNet = ConceptNet()  # ConceptNet toolkit\n",
    "\n",
    "        # Initialization\n",
    "        src_node = conceptNet.get_concept_id(src_word)  # word to concept_id (node id)\n",
    "        tgt_node = conceptNet.get_concept_id(tgt_word)\n",
    "\n",
    "        bfs_src[0].append(src_node)\n",
    "        bfs_tgt[0].append(tgt_node)\n",
    "        weights[0][src_node] = 0\n",
    "        prev_node[0][src_node] = None  # root node\n",
    "        relations[0][src_node] = None\n",
    "\n",
    "        bfs_src[1].append(tgt_node)\n",
    "        bfs_tgt[1].append(src_node)\n",
    "        weights[1][tgt_node] = 0\n",
    "        prev_node[1][tgt_node] = None  # root node\n",
    "        relations[1][tgt_node] = None\n",
    "\n",
    "        for cur_depth in range(max_depth >> 1):\n",
    "            if not continue_bfs_flag:\n",
    "                break\n",
    "            timer_s = time.perf_counter()\n",
    "            # if verbose:\n",
    "            #     print(f\">>> >>> cur_depth: {cur_depth}; len(bfs_src): {len(bfs_src)}\")\n",
    "            if len(bfs_src[0]) == 0 and len(bfs_src[1]) == 0:\n",
    "                break\n",
    "\n",
    "            ignore_node_cnt = [0, 0]\n",
    "\n",
    "            next_bfs = [[], []]  # the nodes of the next depth\n",
    "            for s_idx in range(2):  # search index of the current source nodes\n",
    "                t_set = set(bfs_tgt[s_idx])  # target node set (for matching)\n",
    "                for cur_node in bfs_src[s_idx]:  # deal with all the nodes of the current depth\n",
    "                    # Get the current node (concept id)\n",
    "                    if cur_node not in visited[s_idx]:  # avoid visiting the same node and forming a loop in prev_node\n",
    "                        visited[s_idx].add(cur_node)\n",
    "                    else:\n",
    "                        continue\n",
    "                    # if verbose:\n",
    "                    #     print(f\"{cur_node}; cur_depth: {cur_depth}; len(bfs_src): {len(bfs_src)}\")\n",
    "                    assert isinstance(cur_node, str) and cur_node.startswith(conceptNet.prefix_cid)\n",
    "                    if cur_node in matches_set:\n",
    "                        continue\n",
    "\n",
    "                    # Match the current node with the target word\n",
    "                    # match_result = self.match(tgt_word, cur_node)\n",
    "                    match_result = self.match_set(t_set, cur_node)\n",
    "                    if match_result > 0:  # match the target word\n",
    "                        if cur_node not in matches_set:\n",
    "                            matches_set.add(cur_node)\n",
    "                            matches.append(cur_node)\n",
    "                        # if verbose:\n",
    "                        #     print(f\">>> *** Number {len(matches_set[s_idx])} matched node: {cur_node}\")\n",
    "                        # Once matching a word in the same type/category as the target word, end the Dij process\n",
    "                        continue_bfs_flag = False  # end the whole Dij search process of the next loop\n",
    "                        # break\n",
    "\n",
    "                    # Get the corresponding concept from ConceptNet\n",
    "                    cur_url = conceptNet.get_url(cur_node)\n",
    "                    concept = conceptNet.get_concept(cur_url, verbose=verbose)\n",
    "                    if not isinstance(concept, dict) or \"edges\" not in concept:\n",
    "                        continue\n",
    "\n",
    "                    # Dealing with its neighbors (related concepts)\n",
    "                    edges = concept[\"edges\"]\n",
    "                    if not isinstance(edges, list) or len(edges) == 0:\n",
    "                        continue\n",
    "                    for edge in edges:\n",
    "                        assert isinstance(edge, dict)\n",
    "                        next_node, weight, relation, direction = conceptNet.get_next_node(cur_node, edge)\n",
    "                        if next_node in visited[s_idx] or not next_node.startswith(conceptNet.prefix_cid):\n",
    "                            continue\n",
    "\n",
    "                        # Optimize by ignoring irrelevant nodes based on similarity\n",
    "                        next_node_name = next_node.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "                        next_emb = self.st_model.encode(next_node_name, convert_to_tensor=True)\n",
    "                        if s_idx == 0:  # src-tgt search\n",
    "                            cur_similarity_sent = float(util.pytorch_cos_sim(next_emb, self.tgt_emb))\n",
    "                        else:  # tgt-src search\n",
    "                            cur_similarity_sent = float(util.pytorch_cos_sim(next_emb, self.src_emb))\n",
    "                        if cur_similarity_sent < self.simi_threshold:\n",
    "                            ignore_node_cnt[s_idx] += 1\n",
    "                            continue\n",
    "\n",
    "                        # Add the new node to the path, update the weight.\n",
    "                        if next_node not in weights[s_idx]:\n",
    "                            assert next_node not in prev_node[s_idx], \"Node hasn't been visited.\"\n",
    "                            next_bfs[s_idx].append(next_node)  # next_node will be dealt in the next depth\n",
    "                            weights[s_idx][next_node] = weights[s_idx][cur_node] + weight\n",
    "                            prev_node[s_idx][next_node] = cur_node\n",
    "                            relations[s_idx][next_node] = [relation, direction]\n",
    "                        # If the weight of a new path is greater than the path before, replace the most likely path.\n",
    "                        elif weights[s_idx][next_node] < weights[s_idx][cur_node] + weight:\n",
    "                            assert next_node in prev_node[s_idx], \"Node has been visited.\"\n",
    "                            weights[s_idx][next_node] = weights[s_idx][cur_node] + weight\n",
    "                            prev_node[s_idx][next_node] = cur_node\n",
    "                            relations[s_idx][next_node] = [relation, direction]\n",
    "\n",
    "            timer_e = time.perf_counter()\n",
    "            timer_sec, timer_min = timer_e - timer_s, (timer_e - timer_s) / 60\n",
    "            if verbose:\n",
    "                log_text = f\">>> >>> Depth [{cur_depth + 1}] Time {timer_sec:.1f} sec ({timer_min:.1f} min): \"\n",
    "                log_text += f\"[src->tgt] BFS source {len(bfs_src[0])}; \" \\\n",
    "                            f\"Visited neighbors {len(next_bfs[0])} (ignored {ignore_node_cnt[0]}); \"\n",
    "                log_text += f\"[tgt->src] BFS source {len(bfs_src[1])}; \" \\\n",
    "                            f\"Visited neighbors {len(next_bfs[1])} (ignored {ignore_node_cnt[1]})\"\n",
    "                print(log_text)\n",
    "\n",
    "            cur_match_set = set(next_bfs[0]) & set(next_bfs[1])\n",
    "            if len(cur_match_set) > 0:\n",
    "                for cur_match_node in cur_match_set:\n",
    "                    matches_set.add(cur_match_node)\n",
    "                    matches.append(cur_match_node)\n",
    "                break\n",
    "\n",
    "            bfs_src[0] = next_bfs[0]  # source nodes of the next depth (the src-tgt direction)\n",
    "            bfs_tgt[0] = next_bfs[1]  # target nodes of the next depth (the src-tgt direction)\n",
    "            bfs_src[1] = next_bfs[1]  # source nodes of the next depth (the tgt-src direction)\n",
    "            bfs_tgt[1] = next_bfs[0]  # target nodes of the next depth (the tgt-src direction)\n",
    "\n",
    "        # Return the path list\n",
    "        path_list = []\n",
    "        for matched_node in matches:\n",
    "            path_list.append(self.get_path(matched_node, prev_node, weights, relations))\n",
    "        if verbose:\n",
    "            if len(path_list) == 0:\n",
    "                print(f\">>> Path between {src_word} and {tgt_word} does not exist or is too long (> {max_depth}).\")\n",
    "            else:\n",
    "                print(f\">>> Found {len(path_list)} path(s) between \\\"{src_word}\\\" and \\\"{tgt_word}\\\".\")\n",
    "\n",
    "        return path_list\n",
    "\n",
    "    @staticmethod\n",
    "    def match(tgt_word: str, node: str) -> int:\n",
    "        # string preproc\n",
    "        tgt_word = tgt_word.strip().replace(\"_\", \" \")\n",
    "        node_words = node.split(\"/\")\n",
    "        node_words = [w.strip().replace(\"_\", \" \") for w in node_words]\n",
    "\n",
    "        # match the main concept\n",
    "        if len(node_words) >= 4:  # e.g., \"/c/en/food\"\n",
    "            if tgt_word == node_words[3]:\n",
    "                return 1  # exact match the target word\n",
    "\n",
    "        # match the sub-concept\n",
    "        if len(node_words) >= 7:  # e.g., \"/c/en/butter/n/wn/food\"\n",
    "            if tgt_word == node_words[6]:\n",
    "                return 2  # exact match a word in the same type/category of the target word\n",
    "\n",
    "        return 0  # not matched\n",
    "\n",
    "    @staticmethod\n",
    "    def match_set(tgt_word_set: Set[str], node: str) -> int:\n",
    "        return 1 if node in tgt_word_set else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_path(node: str, prev_node: List[dict], weights: List[dict],\n",
    "                    relations: List[dict], do_print: bool = False) -> Tuple[list, list, list]:\n",
    "        node_list = [[], []]\n",
    "        w_list = [[], []]\n",
    "        r_list = [[], []]\n",
    "        for s_idx in range(2):\n",
    "            cur_node = node\n",
    "            while isinstance(cur_node, str):\n",
    "                node_list[s_idx].append(cur_node)\n",
    "                cur_r = relations[s_idx][cur_node]\n",
    "                r_list[s_idx].append(cur_r)\n",
    "                cur_w = weights[s_idx][cur_node]\n",
    "                w_list[s_idx].append(cur_w)\n",
    "                if do_print:\n",
    "                    print(f\"{node}, with a weight of {cur_w}.\")\n",
    "                    if cur_r[1] == 0:\n",
    "                        print(f\"Backward, relationship is {cur_r[0]}.\")\n",
    "                    else:\n",
    "                        print(f\"Forward, relationship is {cur_r[0]}\")\n",
    "                cur_node = prev_node[s_idx][cur_node]\n",
    "\n",
    "        node_list = list(reversed(node_list[0])) + node_list[1][1:]\n",
    "        w_list = list(reversed(w_list[0][:-1])) + w_list[1][:-1]\n",
    "        r_list = [[item[0], (item[1] + 1) % 2] for item in list(reversed(r_list[0][:-1]))] + r_list[1][:-1]\n",
    "\n",
    "        assert len(node_list) == len(w_list) + 1\n",
    "        return node_list, w_list, r_list\n",
    "\n",
    "    def print_path(self, node: str, prev_node: List[dict], weights: List[dict], relations: List[dict]) -> None:\n",
    "        print(f\"Valid Path:\")\n",
    "        node_list, w_list, r_list = self.get_path(node, prev_node, weights, relations, do_print=True)\n",
    "        print(f\"Path ends. Path Length: {len(w_list)}; Weight Sum: {sum(w_list)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dij = DijkstraSearchBiSource()  # bi-source weighted BFS\n",
    "textParser = TextParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(example, max_number_kw: int = 3, n_bag: int = 1, verbose: bool = False):\n",
    "    # import spacy\n",
    "\n",
    "    # question = example[\"premise\"] + \" \" + question_type_to_nl[example[\"question\"]]  # question text\n",
    "    question = example[\"premise\"]  # question text\n",
    "    choice_key = \"choice\" + str(example[\"label\"] + 1)  # answer key\n",
    "    choice = example[choice_key]  # answer text\n",
    "\n",
    "    # nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # q_nlp = nlp(question)\n",
    "    # c_nlp = nlp(choice)\n",
    "\n",
    "    q_keywords = textParser.get_keywords_keybert(question, n_bag=n_bag)  # get keywords of the question\n",
    "    q_keywords = list(set(q_keywords))  # remove duplication\n",
    "\n",
    "    c_keywords = textParser.get_keywords_keybert(choice, n_bag=n_bag)  # get keywords of the correct choice\n",
    "    c_keywords = list(set(c_keywords))  # remove duplication\n",
    "\n",
    "    # sort c_keywords by computing the average similarity between the current c_kw and each q_kw in q_keywords\n",
    "    c_keywords = textParser.keyword_sort(references=q_keywords, keywords=c_keywords)\n",
    "    c_keywords = c_keywords[:max_number_kw]\n",
    "    q_keywords = q_keywords[:max_number_kw]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"question: {question}\")\n",
    "        print(f\"answer: {choice}\")\n",
    "        print(f\"q_keywords: {q_keywords}\")\n",
    "        print(f\"c_keywords: {c_keywords}\")\n",
    "\n",
    "    return q_keywords, c_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tNIs1HBb7Cwi"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def retrieve_knowledge(\n",
    "    example,\n",
    "    e_id: int,\n",
    "    kw_pairs: Optional[list] = None,\n",
    "    pair_cnt: int = -1,\n",
    "    max_depth: int = 6,\n",
    "    verbose: bool = False,\n",
    ") -> str:\n",
    "    ############################################\n",
    "    #  Complete the following code\n",
    "    ############################################\n",
    "\n",
    "    timer_start_example = time.perf_counter()\n",
    "\n",
    "    if not (isinstance(kw_pairs, list) and len(kw_pairs) > 0):\n",
    "        q_keywords, c_keywords = get_keywords(example, max_number_kw=3, verbose=verbose)\n",
    "\n",
    "        kw_pairs = []\n",
    "        for ck in c_keywords:\n",
    "            for qk in q_keywords:\n",
    "                if ck == qk:  # skip kw_pair that q_keyword == c_keyword (path length will be 0)\n",
    "                    continue\n",
    "                kw_pairs.append((ck, qk))\n",
    "\n",
    "        assert len(kw_pairs) >= 1, f\"Assertion Error: len(kw_pairs) = {len(kw_pairs)}\"\n",
    "        if pair_cnt > 0:\n",
    "            kw_pairs = kw_pairs[: pair_cnt]  # only use the first pair_cnt of kw_pair that q_keyword is most relavant to c_keyword\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Todo kw_pairs: {kw_pairs}\")\n",
    "\n",
    "    best_p_list = []  # best_path and best_config of each Dij search\n",
    "    rationale_dict_list = []\n",
    "    rationale_list = []\n",
    "    rationale_id = 1\n",
    "\n",
    "    for src_word, tgt_word in kw_pairs:\n",
    "        if src_word == tgt_word:\n",
    "            continue\n",
    "\n",
    "        path_list = dij.dijkstra_path_search(src_word, tgt_word, verbose=False, max_depth=max_depth)\n",
    "\n",
    "        if len(path_list) == 0:\n",
    "            continue\n",
    "\n",
    "        # There could be multiple paths from the src_word to tgt_word.\n",
    "        # We select the best path by choosing the path with the largest average edge weights.\n",
    "        best_path = [[], [], []]  # For the current pair, the best path of all returned paths\n",
    "        best_config = [0.0, 0.0, 0.0]  # the best path_len, w_sun, w_avg\n",
    "        for path in path_list:\n",
    "            node_list, w_list, r_list = path\n",
    "            assert len(node_list) == len(w_list) + 1\n",
    "            path_len = len(w_list)\n",
    "            w_sum = sum(w_list)\n",
    "            w_avg = float(w_sum / path_len) if path_len > 0 else 0.0\n",
    "            if w_avg > best_config[2]:  # update the best path\n",
    "                best_path = path\n",
    "                best_config = [path_len, w_sum, w_avg]\n",
    "\n",
    "        best_p_list.append((best_path, best_config))\n",
    "\n",
    "        textConverter = TextConverter(best_path)\n",
    "        data_dict = textConverter.convert(verbose=False)\n",
    "\n",
    "        cur_desc_list = [desc.strip() for desc in data_dict[\"description_list\"]]\n",
    "        cur_rationale = [desc.capitalize() for desc in cur_desc_list]\n",
    "        cur_rationale = \"; \".join(cur_rationale) + \".\"\n",
    "        # cur_rationale = f\"Conceptually, \\\"{src_word}\\\" is related to \\\"{tgt_word}\\\" because: \" + cur_rationale\n",
    "        # cur_rationale = f\"{rationale_id}. \\\"{src_word}\\\" is conceptually related to \\\"{tgt_word}\\\" because: \" + cur_rationale\n",
    "        cur_rationale = f\"\\\"{src_word}\\\" is conceptually related to \\\"{tgt_word}\\\" because: \" + cur_rationale\n",
    "        rationale_id += 1\n",
    "\n",
    "        rationale_dict = {\n",
    "            \"src_word\": src_word,\n",
    "            \"tgt_word\": tgt_word,\n",
    "            \"description_list\": cur_desc_list,\n",
    "            \"rationale\": cur_rationale\n",
    "        }\n",
    "        rationale_dict_list.append(rationale_dict)\n",
    "        rationale_list.append(cur_rationale)\n",
    "\n",
    "    if len(rationale_list) > 0:\n",
    "        # final_rationale = \"\\n\".join(rationale_list)\n",
    "        final_rationale = \" \".join(rationale_list)\n",
    "        print(f\">>> Rationale: {final_rationale}\")\n",
    "    else:\n",
    "        print(f\">>> No rationale (path).\")\n",
    "        final_rationale = \"None.\"\n",
    "\n",
    "    timer_end_example = time.perf_counter()\n",
    "    time_sec, time_min = timer_end_example - timer_start_example, (timer_end_example - timer_start_example) / 60\n",
    "    if verbose:\n",
    "        print(f\"DONE Example {e_id} - Running Time: {time_sec:.1f} sec ({time_min:.1f} min)\\n\")\n",
    "\n",
    "    return final_rationale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuGf3HXB7frW"
   },
   "source": [
    "Add the rationale for each of the in-context and validation set examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: The woman felt lonely.\n",
      "Answer: 2) She adopted a cat.\n",
      "\n",
      "Question: The mother needed help looking after her children.\n",
      "Answer: 1) She sent the children to daycare.\n",
      "\n",
      "Question: I learned how to play the board game.\n",
      "Answer: 1) My friend explained the rules to me.\n",
      "\n",
      "Question: The woman's eyeglasses fogged up.\n",
      "Answer: 2) She entered the sauna.\n",
      "\n",
      "Question: I ran out of breath.\n",
      "Answer: 1) I climbed several flights of stairs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e_id, ex in enumerate(in_context_examples):\n",
    "    question = ex[\"premise\"]  # question text\n",
    "    choice_key = \"choice\" + str(ex[\"label\"] + 1)  # answer key\n",
    "    choice = ex[choice_key]  # answer text\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {str(ex['label'] + 1)}) {choice}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_context_examples_kw_pairs_list = [\n",
    "    [(\"lonely\", \"cat\"), (\"woman\", \"adopt\")],\n",
    "    [(\"mother\", \"children\"), (\"children\", \"daycare\")],\n",
    "    [(\"learn\", \"game\"), (\"game\", \"rules\")],  # (\"learn\", \"rules\")\n",
    "    [(\"eyeglasses\", \"fog\"), (\"fog\", \"sauna\")],\n",
    "    [(\"breath\", \"climb\"), (\"climb\", \"stair\")],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of in_context_examples is 5\n",
      "Todo kw_pairs: [('lonely', 'cat'), ('woman', 'adopt')]\n",
      ">>> Rationale: \"woman\" is conceptually related to \"adopt\" because: Lady is like woman; Lady is like female; Girl is like female; Chick is like girl; Chick is like baby; Baby is like child; Adopt is like child.\n",
      "DONE Example 0 - Running Time: 66.9 sec (1.1 min)\n",
      "\n",
      "Todo kw_pairs: [('mother', 'children'), ('children', 'daycare')]\n",
      ">>> Rationale: \"mother\" is conceptually related to \"children\" because: Daughter is like mother; Daughter is like child; You are likely to find child in school; You are likely to find children in school.\n",
      "DONE Example 1 - Running Time: 24.8 sec (0.4 min)\n",
      "\n",
      "Todo kw_pairs: [('learn', 'game'), ('game', 'rules')]\n",
      ">>> Rationale: \"learn\" is conceptually related to \"game\" because: Something you do when you reading is learn; Sometimes reading causes learning; Playing is used for learning; Toy is like playing; Toy is like fun; Play is like fun; Game is like play. \"game\" is conceptually related to \"rules\" because: Game is like play; Play is like fun; Toy is like fun; Toy is like playing; Card is like playing; Card is like king; Rule is like king; Rule is like law; Law is like rules.\n",
      "DONE Example 2 - Running Time: 27.0 sec (0.5 min)\n",
      "\n",
      "Todo kw_pairs: [('eyeglasses', 'fog'), ('fog', 'sauna')]\n",
      ">>> Rationale: \"eyeglasses\" is conceptually related to \"fog\" because: Pair of glasses and eyeglasses have similar meanings; Pair of glasses is like lens; Lens is part of eye; Eye is like organ; You are likely to find organ in body; Hand is like body; You are likely to find cigarette in hand; Smoke is like cigarette; Smoke is like fog. \"fog\" is conceptually related to \"sauna\" because: Mist is like fog; Mist is like water; Steam is like water; Steam is like air; Air is a type of gas; Oxygen is a type of gas; You are likely to find oxygen in space shuttle; You are likely to find human in space shuttle; You are likely to find human in sauna.\n",
      "DONE Example 3 - Running Time: 106.3 sec (1.8 min)\n",
      "\n",
      "Todo kw_pairs: [('breath', 'climb'), ('climb', 'stair')]\n",
      ">>> Rationale: \"breath\" is conceptually related to \"climb\" because: Breath is like air; Sky is like air; Sky is like up; Climb is like up. \"climb\" is conceptually related to \"stair\" because: Stairs up is used for climb; You are likely to find stairs up in building; You are likely to find staircase in building; The word staircase is derived from the word stair.\n",
      "DONE Example 4 - Running Time: 40.9 sec (0.7 min)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of in_context_examples is {len(in_context_examples)}\")\n",
    "\n",
    "in_context_examples_kb = in_context_examples.copy()\n",
    "\n",
    "assert len(in_context_examples_kw_pairs_list) == len(in_context_examples_kb)\n",
    "for e_id, ex in enumerate(in_context_examples_kb):\n",
    "    # cur_rationale = retrieve_knowledge(ex, e_id, kw_pairs=in_context_examples_kw_pairs_list[e_id], pair_cnt=1, verbose=True)\n",
    "    cur_rationale = retrieve_knowledge(ex, e_id, kw_pairs=in_context_examples_kw_pairs_list[e_id], pair_cnt=2, verbose=True)\n",
    "    ex[\"rationale\"] = cur_rationale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"woman\" is conceptually related to \"adopt\" because: Lady is like woman; Lady is like female; Girl is like female; Chick is like girl; Chick is like baby; Baby is like child; Adopt is like child.\n",
      "\n",
      "\"mother\" is conceptually related to \"children\" because: Daughter is like mother; Daughter is like child; You are likely to find child in school; You are likely to find children in school.\n",
      "\n",
      "\"learn\" is conceptually related to \"game\" because: Something you do when you reading is learn; Sometimes reading causes learning; Playing is used for learning; Toy is like playing; Toy is like fun; Play is like fun; Game is like play. \"game\" is conceptually related to \"rules\" because: Game is like play; Play is like fun; Toy is like fun; Toy is like playing; Card is like playing; Card is like king; Rule is like king; Rule is like law; Law is like rules.\n",
      "\n",
      "\"eyeglasses\" is conceptually related to \"fog\" because: Pair of glasses and eyeglasses have similar meanings; Pair of glasses is like lens; Lens is part of eye; Eye is like organ; You are likely to find organ in body; Hand is like body; You are likely to find cigarette in hand; Smoke is like cigarette; Smoke is like fog. \"fog\" is conceptually related to \"sauna\" because: Mist is like fog; Mist is like water; Steam is like water; Steam is like air; Air is a type of gas; Oxygen is a type of gas; You are likely to find oxygen in space shuttle; You are likely to find human in space shuttle; You are likely to find human in sauna.\n",
      "\n",
      "\"breath\" is conceptually related to \"climb\" because: Breath is like air; Sky is like air; Sky is like up; Climb is like up. \"climb\" is conceptually related to \"stair\" because: Stairs up is used for climb; You are likely to find stairs up in building; You are likely to find staircase in building; The word staircase is derived from the word stair.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e_id, ex in enumerate(in_context_examples_kb):\n",
    "    cur_rationale = ex[\"rationale\"]\n",
    "    print(cur_rationale)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of val_set is 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d69bc3056b40f18297905cc083970b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: The man turned on the faucet.\n",
      "answer: Water flowed from the spout.\n",
      "q_keywords: ['man', 'turned', 'faucet']\n",
      "c_keywords: ['flowed', 'water', 'spout']\n",
      "Todo kw_pairs: [('flowed', 'man'), ('flowed', 'turned')]\n",
      ">>> Rationale: \"flowed\" is conceptually related to \"man\" because: Backwash is like flowed; Backwash is like food; Farmer is like food; Farmer is like man. \"flowed\" is conceptually related to \"turned\" because: Backwash is like flowed; Backwash is like backward; Backward is similar to backward; Transposed is similar to backward; Reversed and transposed have similar meanings; Turned is similar to reversed; Turned and turned have similar meanings.\n",
      "DONE Example 0 - Running Time: 48.5 sec (0.8 min)\n",
      "\n",
      "question: The girl found a bug in her cereal.\n",
      "answer: She lost her appetite.\n",
      "q_keywords: ['cereal', 'girl', 'bug']\n",
      "c_keywords: ['appetite', 'lost']\n",
      "Todo kw_pairs: [('appetite', 'cereal'), ('appetite', 'girl')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 1 - Running Time: 96.8 sec (1.6 min)\n",
      "\n",
      "question: The woman retired.\n",
      "answer: She received her pension.\n",
      "q_keywords: ['woman', 'retired']\n",
      "c_keywords: ['pension', 'received']\n",
      "Todo kw_pairs: [('pension', 'woman'), ('pension', 'retired')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 2 - Running Time: 53.1 sec (0.9 min)\n",
      "\n",
      "question: I wanted to conserve energy.\n",
      "answer: I shut off the light in the unoccupied room.\n",
      "q_keywords: ['energy', 'wanted', 'conserve']\n",
      "c_keywords: ['light', 'shut', 'room']\n",
      "Todo kw_pairs: [('light', 'energy'), ('light', 'wanted')]\n",
      ">>> Rationale: \"light\" is conceptually related to \"energy\" because: Sun is used for light; Sun is used for energy. \"light\" is conceptually related to \"wanted\" because: Shade is like light; Shade is like sun; Sun is like star; You are likely to find star in sky; Blue is like sky; It cannot be both blue and red; Car is red; You are likely to find car in city; You are likely to find restaurant in city; You are likely to find bill in restaurant; Bill does not have the property of wanted.\n",
      "DONE Example 3 - Running Time: 142.1 sec (2.4 min)\n",
      "\n",
      "question: The hamburger meat browned.\n",
      "answer: The cook grilled it.\n",
      "q_keywords: ['browned', 'meat', 'hamburger']\n",
      "c_keywords: ['grilled', 'cook']\n",
      "Todo kw_pairs: [('grilled', 'browned'), ('grilled', 'meat')]\n",
      ">>> Rationale: \"grilled\" is conceptually related to \"meat\" because: Steaks can be grilled; Steaks can be cooked on grill; Dinner can be cooked on grill; Plate is like dinner; Plate is like food; Chicken is a type of food; Chicken is a type of meat.\n",
      "DONE Example 4 - Running Time: 78.0 sec (1.3 min)\n",
      "\n",
      "question: I doubted the salesman's pitch.\n",
      "answer: I turned his offer down.\n",
      "q_keywords: ['pitch', 'doubted', 'salesman']\n",
      "c_keywords: ['turned', 'offer']\n",
      "Todo kw_pairs: [('turned', 'pitch'), ('turned', 'doubted')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 5 - Running Time: 81.8 sec (1.4 min)\n",
      "\n",
      "question: I decided to stay home for the night.\n",
      "answer: The forecast called for storms.\n",
      "q_keywords: ['home', 'decided', 'stay']\n",
      "c_keywords: ['called', 'storms', 'forecast']\n",
      "Todo kw_pairs: [('called', 'home'), ('called', 'decided')]\n",
      ">>> Rationale: \"called\" is conceptually related to \"home\" because: Name is like called; Name is like person; Person is like human; You are likely to find human in school; You are likely to find computer in school; You are likely to find computer in building; House is like building; Nest is like house; Nest is like home.\n",
      "DONE Example 6 - Running Time: 199.5 sec (3.3 min)\n",
      "\n",
      "question: My eyes became red and puffy.\n",
      "answer: I was sobbing.\n",
      "q_keywords: ['puffy', 'red', 'eyes']\n",
      "c_keywords: ['sobbing']\n",
      "Todo kw_pairs: [('sobbing', 'puffy'), ('sobbing', 'red')]\n",
      ">>> Rationale: \"sobbing\" is conceptually related to \"puffy\" because: Cry is like sobbing; Cry is like tears; Tears is like tear; Drop is like tear; Drop is like water; Rain is a type of water; Rain is like sky; Cloud is like sky; Cloud is like puffy. \"sobbing\" is conceptually related to \"red\" because: Cry is like sobbing; Cry is like eyes; Eyes is blue; Blue is like color; Color is like red.\n",
      "DONE Example 7 - Running Time: 63.2 sec (1.1 min)\n",
      "\n",
      "question: The flame on the candle went out.\n",
      "answer: I blew on the wick.\n",
      "q_keywords: ['candle', 'went', 'flame']\n",
      "c_keywords: ['blew', 'wick']\n",
      "Todo kw_pairs: [('blew', 'candle'), ('blew', 'went')]\n",
      ">>> Rationale: \"blew\" is conceptually related to \"candle\" because: Blowed and blew have similar meanings; Blowed is a form of the word blow; Blow is like candles; Wax is like candles; Wax is like candle.\n",
      "DONE Example 8 - Running Time: 22.7 sec (0.4 min)\n",
      "\n",
      "question: The man drank heavily at the party.\n",
      "answer: He had a headache the next day.\n",
      "q_keywords: ['man', 'party', 'heavily']\n",
      "c_keywords: ['day', 'headache']\n",
      "Todo kw_pairs: [('day', 'man'), ('day', 'party')]\n",
      ">>> Rationale: \"day\" is conceptually related to \"man\" because: Morning is like day; Morning is like time; Minute is like time; Minute is like hour; Hour is like clock; You are likely to find clock in house; You are likely to find family in house; Daughter is like family; Daughter is like girl; Girl is like young; Child is like young; Boy is like child; Boy is like man. \"day\" is conceptually related to \"party\" because: Birthday is like day; Birthday is like party.\n",
      "DONE Example 9 - Running Time: 245.6 sec (4.1 min)\n",
      "\n",
      "question: The bowling ball knocked over the bowling pins.\n",
      "answer: The man rolled the bowling ball down the alley.\n",
      "q_keywords: ['bowling', 'ball', 'knocked']\n",
      "c_keywords: ['bowling', 'ball', 'rolled']\n",
      "Todo kw_pairs: [('bowling', 'ball'), ('bowling', 'knocked')]\n",
      ">>> Rationale: \"bowling\" is conceptually related to \"ball\" because: Bowling is a type of sport; Baseball is a type of sport; Baseball is used for playing game; Ball is used for playing game. \"bowling\" is conceptually related to \"knocked\" because: Bowling is a type of sport; Soccer is a type of sport; Knockdown is a word used in the context of soccer; Knockdown is like knocked.\n",
      "DONE Example 10 - Running Time: 28.5 sec (0.5 min)\n",
      "\n",
      "question: The community learned of the man's death.\n",
      "answer: His obituary appeared in the newspaper.\n",
      "q_keywords: ['learned', 'man', 'death']\n",
      "c_keywords: ['appeared', 'newspaper', 'obituary']\n",
      "Todo kw_pairs: [('appeared', 'learned'), ('appeared', 'man')]\n",
      ">>> Rationale: \"appeared\" is conceptually related to \"learned\" because: Appeared is like appear; Appear is like up; Down is the opposite of up; Down is like learned. \"appeared\" is conceptually related to \"man\" because: Appeared is like appear; Appear is like show; Play is like show; Doll is used for play; Doll is like girl; Girl is like young; Child is like young; Boy is like child; Boy is like man.\n",
      "DONE Example 11 - Running Time: 62.8 sec (1.0 min)\n",
      "\n",
      "question: My computer crashed.\n",
      "answer: I lost all my data.\n",
      "q_keywords: ['computer', 'crashed']\n",
      "c_keywords: ['lost', 'data']\n",
      "Todo kw_pairs: [('lost', 'computer'), ('lost', 'crashed')]\n",
      ">>> Rationale: \"lost\" is conceptually related to \"computer\" because: Lost and doomed have similar meanings; Doomed is a type of people; People is a type of family; Family and family have similar meanings; Home is like family; Home is like house; You are likely to find computer in house.\n",
      "DONE Example 12 - Running Time: 218.2 sec (3.6 min)\n",
      "\n",
      "question: The woman resigned from her job.\n",
      "answer: She believed her superiors were acting unethically.\n",
      "q_keywords: ['job', 'woman', 'resigned']\n",
      "c_keywords: ['acting', 'superiors', 'believed']\n",
      "Todo kw_pairs: [('acting', 'job'), ('acting', 'woman')]\n",
      ">>> Rationale: \"acting\" is conceptually related to \"job\" because: Stage is like acting; Play is like stage; It cannot be both play and work; Work is like job. \"acting\" is conceptually related to \"woman\" because: Something you do when you pretending is acting; Something you do when you pretending is religion; God is part of religion; Lord is like god; Lady is like lord; Lady is like woman.\n",
      "DONE Example 13 - Running Time: 108.5 sec (1.8 min)\n",
      "\n",
      "question: The player caught the ball.\n",
      "answer: Her teammate threw it to her.\n",
      "q_keywords: ['player', 'caught', 'ball']\n",
      "c_keywords: ['teammate', 'threw']\n",
      "Todo kw_pairs: [('teammate', 'player'), ('teammate', 'caught')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 14 - Running Time: 36.2 sec (0.6 min)\n",
      "\n",
      "question: The judge pounded the gavel.\n",
      "answer: The courtroom broke into uproar.\n",
      "q_keywords: ['judge', 'pounded', 'gavel']\n",
      "c_keywords: ['courtroom', 'uproar', 'broke']\n",
      "Todo kw_pairs: [('courtroom', 'judge'), ('courtroom', 'pounded')]\n",
      ">>> Rationale: \"courtroom\" is conceptually related to \"pounded\" because: Courtroom is a type of room; Room is like house; You are likely to find furniture in house; Bed is like furniture; Bed is like sleep; Alcohol makes people want sleep; Inebriated is like alcohol; Pounded is like inebriated.\n",
      "DONE Example 15 - Running Time: 132.5 sec (2.2 min)\n",
      "\n",
      "question: The woman banished the children from her property.\n",
      "answer: The children trampled through her garden.\n",
      "q_keywords: ['children', 'woman', 'banished']\n",
      "c_keywords: ['children', 'garden', 'trampled']\n",
      "Todo kw_pairs: [('children', 'woman'), ('children', 'banished')]\n",
      ">>> Rationale: \"children\" is conceptually related to \"woman\" because: Sometimes sex causes children; Baby is created by sex; Chick is like baby; Chick is like girl; Girl is like female; Dress is like female; Dress is like woman.\n",
      "DONE Example 16 - Running Time: 137.4 sec (2.3 min)\n",
      "\n",
      "question: The kidnappers released the hostage.\n",
      "answer: They accepted ransom money.\n",
      "q_keywords: ['released', 'hostage', 'kidnappers']\n",
      "c_keywords: ['ransom', 'money', 'accepted']\n",
      "Todo kw_pairs: [('ransom', 'released'), ('ransom', 'hostage')]\n",
      ">>> Rationale: \"ransom\" is conceptually related to \"released\" because: Ransom and ransom have similar meanings; Ransom is a specific way of doing exchange; Exchange is a specific way of doing transfer; Pass is a specific way of doing transfer; Release is a specific way of doing pass; Release and release have similar meanings; Released is a form of the word release.\n",
      "DONE Example 17 - Running Time: 39.3 sec (0.7 min)\n",
      "\n",
      "question: The cook's eyes watered.\n",
      "answer: He cut an onion.\n",
      "q_keywords: ['watered', 'eyes', 'cook']\n",
      "c_keywords: ['cut', 'onion']\n",
      "Todo kw_pairs: [('cut', 'watered'), ('cut', 'eyes')]\n",
      ">>> Rationale: \"cut\" is conceptually related to \"watered\" because: Cut is like action; Swim is like action; Swim is like water; Watered is like water. \"cut\" is conceptually related to \"eyes\" because: Cut is like action; Sneeze is like action; Sneeze is like nose; Nose is like face; Face is like head; Head is like eyes.\n",
      "DONE Example 18 - Running Time: 51.7 sec (0.9 min)\n",
      "\n",
      "question: The woman ran her finger under cold water.\n",
      "answer: She burned her finger on the toaster.\n",
      "q_keywords: ['finger', 'woman', 'water']\n",
      "c_keywords: ['finger', 'burned', 'toaster']\n",
      "Todo kw_pairs: [('finger', 'woman'), ('finger', 'water')]\n",
      ">>> Rationale: \"finger\" is conceptually related to \"woman\" because: Finger is like hand; Hand is like body; Body is like person; Girl is like person; Girl is like female; Lady is like female; Lady is like woman. \"finger\" is conceptually related to \"water\" because: Finger is like hand; Hand is like arm; Arm is used for wave; Wave is like sea; Lake is like sea; You are likely to find fish in lake; You are likely to find fish in water.\n",
      "DONE Example 19 - Running Time: 63.2 sec (1.1 min)\n",
      "\n",
      "question: The student misspelled the word.\n",
      "answer: The teacher corrected her.\n",
      "q_keywords: ['student', 'word', 'misspelled']\n",
      "c_keywords: ['teacher', 'corrected']\n",
      "Todo kw_pairs: [('teacher', 'student'), ('teacher', 'word')]\n",
      ">>> Rationale: \"teacher\" is conceptually related to \"student\" because: You are likely to find teacher in classroom; You are likely to find student in classroom. \"teacher\" is conceptually related to \"word\" because: You are likely to find teacher in classroom; You are likely to find paper in classroom; Letter is like paper; Letter is part of word.\n",
      "DONE Example 20 - Running Time: 27.5 sec (0.5 min)\n",
      "\n",
      "question: I regained composure from my fit of anger.\n",
      "answer: I took deep breaths.\n",
      "q_keywords: ['composure', 'regained', 'anger']\n",
      "c_keywords: ['breaths', 'took', 'deep']\n",
      "Todo kw_pairs: [('breaths', 'composure'), ('breaths', 'regained')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 21 - Running Time: 18.6 sec (0.3 min)\n",
      "\n",
      "question: I put my hands under the running faucet.\n",
      "answer: The soap rinsed off my hands.\n",
      "q_keywords: ['running', 'faucet', 'hands']\n",
      "c_keywords: ['hands', 'rinsed', 'soap']\n",
      "Todo kw_pairs: [('hands', 'running'), ('hands', 'faucet')]\n",
      ">>> Rationale: \"hands\" is conceptually related to \"running\" because: Hands is a type of body parts; Feet is like body parts; Sock is like feet; Sock is like foot; Foot is used for running. \"hands\" is conceptually related to \"faucet\" because: Clock is like hands; You are likely to find clock in house; You are likely to find bathroom in house; You are likely to find faucet in bathroom.\n",
      "DONE Example 22 - Running Time: 45.6 sec (0.8 min)\n",
      "\n",
      "question: The man dressed in his best suit.\n",
      "answer: He scheduled a meeting with an important client.\n",
      "q_keywords: ['suit', 'dressed', 'man']\n",
      "c_keywords: ['important', 'meeting', 'scheduled']\n",
      "Todo kw_pairs: [('important', 'suit'), ('important', 'dressed')]\n",
      ">>> Rationale: \"important\" is conceptually related to \"suit\" because: Capital is like important; Capital is like city; City is a type of place; Bank is like place; Bank is like holder; Case is like holder; Case is like suit. \"important\" is conceptually related to \"dressed\" because: Capital is like important; Capital is like city; City is a type of place; Court is like place; Court is like legal; Vested interest is a word used in the context of legal; The word vested interest is derived from the word vested; Vested is like dressed.\n",
      "DONE Example 23 - Running Time: 171.0 sec (2.9 min)\n",
      "\n",
      "question: The man confessed his love for the woman.\n",
      "answer: The woman rejected him.\n",
      "q_keywords: ['man', 'woman', 'love']\n",
      "c_keywords: ['woman', 'rejected']\n",
      "Todo kw_pairs: [('woman', 'man'), ('woman', 'love')]\n",
      ">>> Rationale: \"woman\" is conceptually related to \"man\" because: Person is like woman; Man is like person. \"woman\" is conceptually related to \"love\" because: Dress is like woman; Dress is like female; Girl is like female; Chick is like girl; Chick is like baby; Baby is like human; You are likely to find human in love.\n",
      "DONE Example 24 - Running Time: 32.7 sec (0.5 min)\n",
      "\n",
      "question: The driver got a flat tire.\n",
      "answer: He ran over a nail.\n",
      "q_keywords: ['driver', 'tire', 'got']\n",
      "c_keywords: ['ran', 'nail']\n",
      "Todo kw_pairs: [('ran', 'driver'), ('ran', 'tire')]\n",
      ">>> Rationale: \"ran\" is conceptually related to \"tire\" because: Ran is a form of the word run; Run is like walk; Walk is like moving; Drive is like moving; Drive is like car; Tire is part of car.\n",
      "DONE Example 25 - Running Time: 105.1 sec (1.8 min)\n",
      "\n",
      "question: My view of the movie screen was blocked.\n",
      "answer: A tall person was sitting in front of me.\n",
      "q_keywords: ['blocked', 'view', 'screen']\n",
      "c_keywords: ['person', 'sitting', 'tall']\n",
      "Todo kw_pairs: [('person', 'blocked'), ('person', 'view')]\n",
      ">>> Rationale: \"person\" is conceptually related to \"blocked\" because: Person is like human; Doll is like human; Doll is like baby; Baby is like child; Child is like small; Squirrel is like small; Squirrel is like tree; Shade is like tree; Shade is like blocked. \"person\" is conceptually related to \"view\" because: Person is like human; Body is like human; Eye is like body; Eye is like sight; Sight is like see; Look is like see; View is like look.\n",
      "DONE Example 26 - Running Time: 205.6 sec (3.4 min)\n",
      "\n",
      "question: The driver turned on the car's headlights.\n",
      "answer: The sun went down.\n",
      "q_keywords: ['headlights', 'car', 'turned']\n",
      "c_keywords: ['went', 'sun']\n",
      "Todo kw_pairs: [('went', 'headlights'), ('went', 'car')]\n",
      ">>> Rationale: \"went\" is conceptually related to \"car\" because: Yode is like went; Yode is a form of the word go; It cannot be both stop and go; Stop is like red; Car is red.\n",
      "DONE Example 27 - Running Time: 53.9 sec (0.9 min)\n",
      "\n",
      "question: The girl refused to eat her vegetables.\n",
      "answer: Her father took away her dessert.\n",
      "q_keywords: ['eat', 'refused', 'vegetables']\n",
      "c_keywords: ['dessert', 'took', 'father']\n",
      "Todo kw_pairs: [('dessert', 'eat'), ('dessert', 'refused')]\n",
      ">>> Rationale: \"dessert\" is conceptually related to \"eat\" because: Cake is like dessert; Cake is like round; Plate is like round; Plate is like food; You would sate hunger because you want to food; In order for sate hunger to happen, find food needs to happen; In order for diminish own hunger to happen, find food needs to happen; In order for diminish own hunger to happen, eat needs to happen.\n",
      "DONE Example 28 - Running Time: 66.4 sec (1.1 min)\n",
      "\n",
      "question: The woman covered her mouth with her hand.\n",
      "answer: She sneezed.\n",
      "q_keywords: ['woman', 'covered', 'hand']\n",
      "c_keywords: ['sneezed']\n",
      "Todo kw_pairs: [('sneezed', 'woman'), ('sneezed', 'covered')]\n",
      ">>> Rationale: \"sneezed\" is conceptually related to \"woman\" because: Sneezed is like sneeze; Sneeze is like action; Drop is like action; Drop is like small; Baby is like small; Baby is like human; Child is like human; Boy is like child; Boy is like young; Chick is like young; Chick is like girl; Girl is like female; Dress is like female; Dress is like woman.\n",
      "DONE Example 29 - Running Time: 127.1 sec (2.1 min)\n",
      "\n",
      "question: The secretary put the caller on hold.\n",
      "answer: The caller waited on the line.\n",
      "q_keywords: ['hold', 'secretary', 'caller']\n",
      "c_keywords: ['caller', 'line', 'waited']\n",
      "Todo kw_pairs: [('caller', 'hold'), ('caller', 'secretary')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 30 - Running Time: 65.8 sec (1.1 min)\n",
      "\n",
      "question: The woman walked with crutches.\n",
      "answer: She broke her leg.\n",
      "q_keywords: ['crutches', 'woman', 'walked']\n",
      "c_keywords: ['leg', 'broke']\n",
      "Todo kw_pairs: [('leg', 'crutches'), ('leg', 'woman')]\n",
      ">>> Rationale: \"leg\" is conceptually related to \"woman\" because: Leg is like foot; Foot is like body; Body is like human; Person is like human; Girl is like person; Girl is like female; Dress is like female; Dress is like woman.\n",
      "DONE Example 31 - Running Time: 30.3 sec (0.5 min)\n",
      "\n",
      "question: I coughed.\n",
      "answer: I inhaled smoke.\n",
      "q_keywords: ['coughed']\n",
      "c_keywords: ['inhaled', 'smoke']\n",
      "Todo kw_pairs: [('inhaled', 'coughed'), ('smoke', 'coughed')]\n",
      ">>> Rationale: \"smoke\" is conceptually related to \"coughed\" because: Smoke is like air; In order for breathing to happen, air needs to happen; Something you do when you breathing is cough; Coughed is like cough.\n",
      "DONE Example 32 - Running Time: 21.0 sec (0.3 min)\n",
      "\n",
      "question: The clock chimed.\n",
      "answer: It was the top of the hour.\n",
      "q_keywords: ['chimed', 'clock']\n",
      "c_keywords: ['hour']\n",
      "Todo kw_pairs: [('hour', 'chimed'), ('hour', 'clock')]\n",
      ">>> Rationale: \"hour\" is conceptually related to \"clock\" because: Hour is like time; Clock is like time.\n",
      "DONE Example 33 - Running Time: 102.4 sec (1.7 min)\n",
      "\n",
      "question: The chef hit the egg on the side of the bowl.\n",
      "answer: The egg cracked.\n",
      "q_keywords: ['chef', 'bowl', 'hit']\n",
      "c_keywords: ['egg', 'cracked']\n",
      "Todo kw_pairs: [('egg', 'chef'), ('egg', 'bowl')]\n",
      ">>> Rationale: \"egg\" is conceptually related to \"chef\" because: Egg is like chicken; Chicken is like bird; Nest is like bird; Nest is like baby; Baby is like person; Person can cook dinner; Chef can cook dinner. \"egg\" is conceptually related to \"bowl\" because: Nest is like egg; Nest is like bowl.\n",
      "DONE Example 34 - Running Time: 25.8 sec (0.4 min)\n",
      "\n",
      "question: The police searched the offender's car.\n",
      "answer: They were looking for illegal drugs.\n",
      "q_keywords: ['police', 'offender', 'car']\n",
      "c_keywords: ['looking', 'illegal', 'drugs']\n",
      "Todo kw_pairs: [('looking', 'police'), ('looking', 'offender')]\n",
      ">>> Rationale: \"looking\" is conceptually related to \"police\" because: Eye is used for looking; Eye is like head; Chief is like head; Chief is like police. \"looking\" is conceptually related to \"offender\" because: Eye is used for looking; Eye is like organ; You are likely to find organ in body; Body is like person; Person does not want offended; Offender is like offended; The word offender is derived from the word offender.\n",
      "DONE Example 35 - Running Time: 87.5 sec (1.5 min)\n",
      "\n",
      "question: The couple travelled south for the winter.\n",
      "answer: They were retired.\n",
      "q_keywords: ['couple', 'winter', 'travelled']\n",
      "c_keywords: ['retired']\n",
      "Todo kw_pairs: [('retired', 'couple'), ('retired', 'winter')]\n",
      ">>> Rationale: \"retired\" is conceptually related to \"couple\" because: Unretired is like retired; Unretired is like work; Desk is like work; You are likely to find feet in desk; Feet is like two; Couple is two. \"retired\" is conceptually related to \"winter\" because: Sunbird is like retired; Sunbird is like bird; Chick is like bird; Chick is like hot; Summer is like hot; Summer is like season; Winter is like season.\n",
      "DONE Example 36 - Running Time: 141.7 sec (2.4 min)\n",
      "\n",
      "question: The man felt obligated to attend the event.\n",
      "answer: He promised his friend that he would go.\n",
      "q_keywords: ['felt', 'event', 'attend']\n",
      "c_keywords: ['promised', 'friend']\n",
      "Todo kw_pairs: [('promised', 'felt'), ('promised', 'event')]\n",
      ">>> Rationale: \"promised\" is conceptually related to \"event\" because: Promised is like marriage; Sometimes proposing to woman causes marriage; In order for proposing to woman to happen, ring needs to happen; Ring is like round; Cake is like round; Cake is like birthday; Birthday is like party; Party is like event.\n",
      "DONE Example 37 - Running Time: 106.1 sec (1.8 min)\n",
      "\n",
      "question: The bride got cold feet before the wedding.\n",
      "answer: She called the wedding off.\n",
      "q_keywords: ['wedding', 'bride', 'cold']\n",
      "c_keywords: ['wedding', 'called']\n",
      "Todo kw_pairs: [('wedding', 'bride'), ('wedding', 'cold')]\n",
      ">>> Rationale: \"wedding\" is conceptually related to \"cold\" because: Cake is like wedding; Cake is like desert; Desert is like hot; Hot is like temperature; Cold is like temperature.\n",
      "DONE Example 38 - Running Time: 28.8 sec (0.5 min)\n",
      "\n",
      "question: The man grew old.\n",
      "answer: His hair turned gray.\n",
      "q_keywords: ['man', 'grew', 'old']\n",
      "c_keywords: ['hair', 'turned', 'gray']\n",
      "Todo kw_pairs: [('hair', 'man'), ('hair', 'grew')]\n",
      ">>> Rationale: \"hair\" is conceptually related to \"man\" because: You are likely to find hair in head; Head is like body; Body is like person; Boy is like person; Boy is like man. \"hair\" is conceptually related to \"grew\" because: Wool is like hair; Wool is like sheep; Sheep is like animal; Squirrel is like animal; Squirrel is like tree; Tree can grow; Grew is like grow.\n",
      "DONE Example 39 - Running Time: 130.2 sec (2.2 min)\n",
      "\n",
      "question: The friends decided to share the hamburger.\n",
      "answer: They cut the hamburger in half.\n",
      "q_keywords: ['friends', 'decided', 'share']\n",
      "c_keywords: ['hamburger', 'cut', 'half']\n",
      "Todo kw_pairs: [('hamburger', 'friends'), ('hamburger', 'decided')]\n",
      ">>> Rationale: \"hamburger\" is conceptually related to \"friends\" because: Hamburger is a type of food; Cake is like food; Cake is like party; Party is like friends.\n",
      "DONE Example 40 - Running Time: 105.0 sec (1.8 min)\n",
      "\n",
      "question: I twisted the cap off the soda bottle.\n",
      "answer: The soda fizzed.\n",
      "q_keywords: ['twisted', 'cap', 'soda']\n",
      "c_keywords: ['soda', 'fizzed']\n",
      "Todo kw_pairs: [('soda', 'twisted'), ('soda', 'cap')]\n",
      ">>> Rationale: \"soda\" is conceptually related to \"twisted\" because: Coke is a type of soda; Coke is a type of beverage; Wine is a type of beverage; Wine is like red; Blood is like red; Wound is like blood; Twisted is like wound.\n",
      "DONE Example 41 - Running Time: 132.9 sec (2.2 min)\n",
      "\n",
      "question: The pair of students came under scrutiny by the teacher.\n",
      "answer: Their responses on the assignment were identical.\n",
      "q_keywords: ['teacher', 'scrutiny', 'students']\n",
      "c_keywords: ['assignment', 'responses', 'identical']\n",
      "Todo kw_pairs: [('assignment', 'teacher'), ('assignment', 'scrutiny')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 42 - Running Time: 85.2 sec (1.4 min)\n",
      "\n",
      "question: The student was in a rush to get to school on time.\n",
      "answer: He left his assignment at home.\n",
      "q_keywords: ['student', 'school', 'time']\n",
      "c_keywords: ['home', 'assignment', 'left']\n",
      "Todo kw_pairs: [('home', 'student'), ('home', 'school')]\n",
      ">>> Rationale: \"home\" is conceptually related to \"student\" because: Nest is like home; Nest is like house; House is like building; School is like building; College is a type of school; College is like university; You are likely to find student in university. \"home\" is conceptually related to \"school\" because: Home is like place; School is like place.\n",
      "DONE Example 43 - Running Time: 43.8 sec (0.7 min)\n",
      "\n",
      "question: The journalist wrote a biography about the humanitarian's life.\n",
      "answer: The journalist was intrigued by the humanitarian's work.\n",
      "q_keywords: ['journalist', 'biography', 'humanitarian']\n",
      "c_keywords: ['journalist', 'humanitarian', 'work']\n",
      "Todo kw_pairs: [('journalist', 'biography'), ('journalist', 'humanitarian')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 44 - Running Time: 29.8 sec (0.5 min)\n",
      "\n",
      "question: The man defied the authorities of the church.\n",
      "answer: He was excommunicated from the church.\n",
      "q_keywords: ['church', 'man', 'authorities']\n",
      "c_keywords: ['church', 'excommunicated']\n",
      "Todo kw_pairs: [('church', 'man'), ('church', 'authorities')]\n",
      ">>> Rationale: \"church\" is conceptually related to \"man\" because: Cathedral is like church; Cathedral is like city; City is a type of place; Farm is like place; Farmer is like farm; Farmer is like man.\n",
      "DONE Example 45 - Running Time: 170.6 sec (2.8 min)\n",
      "\n",
      "question: The woman's hair fell in her face.\n",
      "answer: She pulled her hair back with a clip.\n",
      "q_keywords: ['face', 'woman', 'fell']\n",
      "c_keywords: ['hair', 'clip', 'pulled']\n",
      "Todo kw_pairs: [('hair', 'face'), ('hair', 'woman')]\n",
      ">>> Rationale: \"hair\" is conceptually related to \"face\" because: You are likely to find hair in head; Face is like head. \"hair\" is conceptually related to \"woman\" because: You are likely to find hair in head; Head is like body; Body is like person; Girl is like person; Girl is like female; Dress is like female; Dress is like woman.\n",
      "DONE Example 46 - Running Time: 39.4 sec (0.7 min)\n",
      "\n",
      "question: The ring on my finger got stuck.\n",
      "answer: My finger swelled.\n",
      "q_keywords: ['finger', 'stuck', 'got']\n",
      "c_keywords: ['finger', 'swelled']\n",
      "Todo kw_pairs: [('finger', 'stuck'), ('finger', 'got')]\n",
      ">>> Rationale: \"finger\" is conceptually related to \"got\" because: Finger is like hand; Hand is like fingers; Arm is like fingers; Arm is like body; Eye is like body; Eye is like head; You are likely to find ear in head; Ear is like organ; You are likely to find stop in organ; It cannot be both stop and go; Go is like got.\n",
      "DONE Example 47 - Running Time: 312.2 sec (5.2 min)\n",
      "\n",
      "question: I pulled the rubber band.\n",
      "answer: It stretched.\n",
      "q_keywords: ['rubber', 'pulled', 'band']\n",
      "c_keywords: ['stretched']\n",
      "Todo kw_pairs: [('stretched', 'rubber'), ('stretched', 'pulled')]\n",
      ">>> Rationale: \"stretched\" is conceptually related to \"pulled\" because: Stretched is similar to extended; Extended is similar to figurative; Extended is similar to figurative; Extended is like pulled.\n",
      "DONE Example 48 - Running Time: 35.4 sec (0.6 min)\n",
      "\n",
      "question: I pressed my hand into the wet cement.\n",
      "answer: My handprint dried in the cement.\n",
      "q_keywords: ['pressed', 'wet', 'cement']\n",
      "c_keywords: ['cement', 'dried', 'handprint']\n",
      "Todo kw_pairs: [('cement', 'pressed'), ('cement', 'wet')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 49 - Running Time: 118.9 sec (2.0 min)\n",
      "\n",
      "question: My skin broke out into a rash.\n",
      "answer: I brushed against poison ivy in my yard.\n",
      "q_keywords: ['skin', 'broke', 'rash']\n",
      "c_keywords: ['poison', 'brushed', 'ivy']\n",
      "Todo kw_pairs: [('poison', 'skin'), ('poison', 'broke')]\n",
      ">>> Rationale: \"poison\" is conceptually related to \"skin\" because: Poison is like liquid; Blood is like liquid; Blood is like body; Body is like skin. \"poison\" is conceptually related to \"broke\" because: Poison is like liquid; Drink is like liquid; Drink is like glass; Glass can be broken; Broke is like broken.\n",
      "DONE Example 50 - Running Time: 42.1 sec (0.7 min)\n",
      "\n",
      "question: My subscription to the magazine expired.\n",
      "answer: I stopped receiving new issues.\n",
      "q_keywords: ['expired', 'subscription', 'magazine']\n",
      "c_keywords: ['issues', 'stopped', 'new']\n",
      "Todo kw_pairs: [('issues', 'expired'), ('issues', 'subscription')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 51 - Running Time: 50.8 sec (0.8 min)\n",
      "\n",
      "question: The detective revealed an anomaly in the case.\n",
      "answer: He scrapped his theory.\n",
      "q_keywords: ['anomaly', 'case', 'revealed']\n",
      "c_keywords: ['theory', 'scrapped']\n",
      "Todo kw_pairs: [('theory', 'anomaly'), ('theory', 'case')]\n",
      ">>> Rationale: \"theory\" is conceptually related to \"case\" because: Theory is like science; You are likely to find science in university; College is like university; College is a type of place; Bank is like place; Bank is like storage; Box is like storage; Box is like container; Case is like container.\n",
      "DONE Example 52 - Running Time: 153.7 sec (2.6 min)\n",
      "\n",
      "question: The boy threw a temper tantrum.\n",
      "answer: His brother took his toys from him.\n",
      "q_keywords: ['tantrum', 'boy', 'temper']\n",
      "c_keywords: ['brother', 'took', 'toys']\n",
      "Todo kw_pairs: [('brother', 'tantrum'), ('brother', 'boy')]\n",
      ">>> Rationale: \"brother\" is conceptually related to \"tantrum\" because: Brother is like sibling; Sister is like sibling; Sister is like family; Daughter is like family; Daughter is like girl; Girl is like female; It cannot be both man and female; Man is like person; Person is like human; You are likely to find human in love; Love is like emotion; Anger is a type of emotion; Hissy fit is like anger; Hissy fit is like tantrum. \"brother\" is conceptually related to \"boy\" because: Brother is like male; He is like male; He is like man; Boy is like man.\n",
      "DONE Example 53 - Running Time: 139.6 sec (2.3 min)\n",
      "\n",
      "question: The child learned how to read.\n",
      "answer: He began attending school.\n",
      "q_keywords: ['learned', 'read', 'child']\n",
      "c_keywords: ['school', 'began', 'attending']\n",
      "Todo kw_pairs: [('school', 'learned'), ('school', 'read')]\n",
      ">>> Rationale: \"school\" is conceptually related to \"learned\" because: You are likely to find human in school; Child is like human; Toy is like child; Toy is like kids; Play is like kids; Play is like fun; Something you do when you play games is fun; Something you do when you play games is lose; Defeat is like lose; Down is like defeat; Down is like learned. \"school\" is conceptually related to \"read\" because: Class is like school; Class is like learning; Sometimes reading causes learning; Literature is used for reading; Literature is like books; Books can be read.\n",
      "DONE Example 54 - Running Time: 207.6 sec (3.5 min)\n",
      "\n",
      "question: The boy skipped dinner.\n",
      "answer: He ate a big lunch.\n",
      "q_keywords: ['dinner', 'boy', 'skipped']\n",
      "c_keywords: ['ate', 'lunch', 'big']\n",
      "Todo kw_pairs: [('ate', 'dinner'), ('ate', 'boy')]\n",
      ">>> Rationale: \"ate\" is conceptually related to \"boy\" because: Apple is like ate; Apple is like fruit; You are likely to find fruit in tree; Nest is like tree; Nest is like house; Home is like house; Home is like family; Sister is like family; Sister is like female; Chick is like female; Chick is like baby; Baby is like human; Person is like human; Girl is like person; Girl is like young; Child is like young; Boy is like child.\n",
      "DONE Example 55 - Running Time: 213.2 sec (3.6 min)\n",
      "\n",
      "question: The woman lavished her friend with flattery.\n",
      "answer: She wanted to ask her friend for a favor.\n",
      "q_keywords: ['woman', 'friend', 'flattery']\n",
      "c_keywords: ['friend', 'favor', 'wanted']\n",
      "Todo kw_pairs: [('friend', 'woman'), ('friend', 'flattery')]\n",
      ">>> Rationale: \"friend\" is conceptually related to \"woman\" because: Friend is like person; Person is like woman.\n",
      "DONE Example 56 - Running Time: 72.9 sec (1.2 min)\n",
      "\n",
      "question: The key was missing from my pants pocket.\n",
      "answer: The pocket had a hole.\n",
      "q_keywords: ['key', 'missing', 'pocket']\n",
      "c_keywords: ['pocket', 'hole']\n",
      "Todo kw_pairs: [('pocket', 'key'), ('pocket', 'missing')]\n",
      ">>> Rationale: \"pocket\" is conceptually related to \"key\" because: Wallet is like pocket; Wallet is like money; Change is like money; Coin is like change; You are likely to find coin in purse; You are likely to find key in purse.\n",
      "DONE Example 57 - Running Time: 187.7 sec (3.1 min)\n",
      "\n",
      "question: The man fainted.\n",
      "answer: He ran a marathon.\n",
      "q_keywords: ['man', 'fainted']\n",
      "c_keywords: ['ran', 'marathon']\n",
      "Todo kw_pairs: [('ran', 'man'), ('ran', 'fainted')]\n",
      ">>> Rationale: \"ran\" is conceptually related to \"man\" because: Rcn is like ran; Rcn is like royal; Royal is like king; King is like man.\n",
      "DONE Example 58 - Running Time: 33.5 sec (0.6 min)\n",
      "\n",
      "question: The man lost the competition.\n",
      "answer: The competition was sabotaged.\n",
      "q_keywords: ['man', 'lost', 'competition']\n",
      "c_keywords: ['competition', 'sabotaged']\n",
      "Todo kw_pairs: [('competition', 'man'), ('competition', 'lost')]\n",
      ">>> Rationale: \"competition\" is conceptually related to \"man\" because: Playing sports is used for competition; Playing sports is used for fun; Party is like fun; Party is like group; Family is like group; Daughter is like family; Daughter is like girl; Girl is like young; Child is like young; Boy is like child; Boy is like man.\n",
      "DONE Example 59 - Running Time: 207.3 sec (3.5 min)\n",
      "\n",
      "question: The mother called an ambulance.\n",
      "answer: Her son fell out of his bed.\n",
      "q_keywords: ['ambulance', 'called', 'mother']\n",
      "c_keywords: ['son', 'bed', 'fell']\n",
      "Todo kw_pairs: [('son', 'ambulance'), ('son', 'called')]\n",
      ">>> Rationale: \"son\" is conceptually related to \"called\" because: Daughter is like son; Daughter is like child; Boy is like child; Boy is like person; Name is like person; Name is like called.\n",
      "DONE Example 60 - Running Time: 60.7 sec (1.0 min)\n",
      "\n",
      "question: The driver slammed on his brakes.\n",
      "answer: A deer appeared on the road.\n",
      "q_keywords: ['brakes', 'driver', 'slammed']\n",
      "c_keywords: ['road', 'appeared', 'deer']\n",
      "Todo kw_pairs: [('road', 'brakes'), ('road', 'driver')]\n",
      ">>> Rationale: \"road\" is conceptually related to \"brakes\" because: You are likely to find car in road; Brakes is part of car.\n",
      "DONE Example 61 - Running Time: 50.9 sec (0.8 min)\n",
      "\n",
      "question: The lock opened.\n",
      "answer: I turned the key in the lock.\n",
      "q_keywords: ['lock', 'opened']\n",
      "c_keywords: ['lock', 'key', 'turned']\n",
      "Todo kw_pairs: [('lock', 'opened'), ('key', 'lock')]\n",
      ">>> Rationale: \"lock\" is conceptually related to \"opened\" because: You are likely to find lock in door; Door can be opened. \"key\" is conceptually related to \"lock\" because: Key is used for opening doors; Knob is used for opening doors; Door is like knob; You are likely to find lock in door.\n",
      "DONE Example 62 - Running Time: 12.2 sec (0.2 min)\n",
      "\n",
      "question: I put rubber gloves on.\n",
      "answer: I was preparing to clean the bathroom.\n",
      "q_keywords: ['gloves', 'rubber']\n",
      "c_keywords: ['clean', 'preparing', 'bathroom']\n",
      "Todo kw_pairs: [('clean', 'gloves'), ('clean', 'rubber')]\n",
      ">>> Rationale: \"clean\" is conceptually related to \"gloves\" because: Cloth is used for clean; Wool is like cloth; Wool is like clothing; Clothing is like gloves. \"clean\" is conceptually related to \"rubber\" because: Cloth is used for clean; Napkin is like cloth; Napkin is like face; Eye is like face; Eye is like sphere; Sphere is round; Ball is like round; Ball is like rubber.\n",
      "DONE Example 63 - Running Time: 79.7 sec (1.3 min)\n",
      "\n",
      "question: The animal species became endangered.\n",
      "answer: Their habitat was destroyed.\n",
      "q_keywords: ['endangered', 'animal', 'species']\n",
      "c_keywords: ['habitat', 'destroyed']\n",
      "Todo kw_pairs: [('habitat', 'endangered'), ('habitat', 'animal')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 64 - Running Time: 82.2 sec (1.4 min)\n",
      "\n",
      "question: The man perceived that the woman looked different.\n",
      "answer: The woman got her hair cut.\n",
      "q_keywords: ['woman', 'looked', 'perceived']\n",
      "c_keywords: ['woman', 'hair', 'got']\n",
      "Todo kw_pairs: [('woman', 'looked'), ('woman', 'perceived')]\n",
      ">>> Rationale: \"woman\" is conceptually related to \"looked\" because: Dress is like woman; Dress is like female; Girl is like female; Chick is like girl; Chick is like young; Boy is like young; Boy is like child; Child is like human; Baby is like human; Baby is like small; Drop is like small; Drop is like down; The word downlooked is derived from the word down; The word downlooked is derived from the word looked. \"woman\" is conceptually related to \"perceived\" because: Dress is like woman; Dress is like female; Girl is like female; Chick is like girl; Chick is like young; Boy is like young; Boy is like child; Child is like human; Baby is like human; Baby is like small; Boat is like small; Boat is like vehicle; Thinking distance is like vehicle; Thinking distance is like perceived.\n",
      "DONE Example 65 - Running Time: 218.1 sec (3.6 min)\n",
      "\n",
      "question: The student forgot to do her assignment.\n",
      "answer: She made up an excuse to tell the teacher.\n",
      "q_keywords: ['student', 'forgot', 'assignment']\n",
      "c_keywords: ['teacher', 'excuse', 'tell']\n",
      "Todo kw_pairs: [('teacher', 'student'), ('teacher', 'forgot')]\n",
      ">>> Rationale: \"teacher\" is conceptually related to \"student\" because: You are likely to find teacher in classroom; You are likely to find student in classroom. \"teacher\" is conceptually related to \"forgot\" because: You are likely to find teacher in classroom; You are likely to find book in classroom; Book is used for learning; Sometimes reading causes learning; Something you do when you studying for subject is reading; Something you do when you studying for subject is forget; Forgot is like forget.\n",
      "DONE Example 66 - Running Time: 86.7 sec (1.4 min)\n",
      "\n",
      "question: The dog barked.\n",
      "answer: A knock sounded at the door.\n",
      "q_keywords: ['barked', 'dog']\n",
      "c_keywords: ['door', 'sounded', 'knock']\n",
      "Todo kw_pairs: [('door', 'barked'), ('door', 'dog')]\n",
      ">>> Rationale: \"door\" is conceptually related to \"barked\" because: Door is like opening; Window is like opening; Window is like wall; Paint is like wall; Paint is like color; Color is like shade; Shade is like tree; Bark is part of tree; Barked is a form of the word bark. \"door\" is conceptually related to \"dog\" because: Door is like house; Home is like house; Nest is like home; Nest is like baby; Baby is like small; Squirrel is like small; Squirrel is like tree; Bark is part of tree; Dog can bark.\n",
      "DONE Example 67 - Running Time: 163.7 sec (2.7 min)\n",
      "\n",
      "question: Plans were announced to replace a local park with a shopping mall.\n",
      "answer: Environmentalists started a petition.\n",
      "q_keywords: ['plans', 'park', 'replace']\n",
      "c_keywords: ['started', 'petition', 'environmentalists']\n",
      "Todo kw_pairs: [('started', 'plans'), ('started', 'park')]\n",
      ">>> Rationale: \"started\" is conceptually related to \"park\" because: The word unstarted is derived from the word started; Unstarted and unbegun have similar meanings; Uncommenced is like unbegun; Uncommenced is a word used in the context of legal; Count is like legal; Count is like balls; You are likely to find balls in park.\n",
      "DONE Example 68 - Running Time: 189.6 sec (3.2 min)\n",
      "\n",
      "question: The couple was happy to see each other.\n",
      "answer: They kissed.\n",
      "q_keywords: ['happy', 'couple']\n",
      "c_keywords: ['kissed']\n",
      "Todo kw_pairs: [('kissed', 'happy'), ('kissed', 'couple')]\n",
      ">>> Rationale: \"kissed\" is conceptually related to \"happy\" because: The word unkissed is derived from the word kissed; Unkissed is like kiss; Love makes people want kiss; Love is like emotion; Glad is like emotion; Glad is like happy. \"kissed\" is conceptually related to \"couple\" because: Kissed is a form of the word kiss; Something you do when you making love is kiss; Sex can making love; Sex is a type of activity; Play is like activity; Play is like performance; Stage is like performance; Stage is like theater; Theater is used for watch movie; Couple can watch movie.\n",
      "DONE Example 69 - Running Time: 96.9 sec (1.6 min)\n",
      "\n",
      "question: The woman asked the man to leave.\n",
      "answer: He insulted her.\n",
      "q_keywords: ['asked', 'leave', 'woman']\n",
      "c_keywords: ['insulted']\n",
      "Todo kw_pairs: [('insulted', 'asked'), ('insulted', 'leave')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 70 - Running Time: 60.7 sec (1.0 min)\n",
      "\n",
      "question: The tree branch landed in the river.\n",
      "answer: The branch moved downstream.\n",
      "q_keywords: ['river', 'branch', 'landed']\n",
      "c_keywords: ['branch', 'downstream', 'moved']\n",
      "Todo kw_pairs: [('branch', 'river'), ('branch', 'landed')]\n",
      ">>> Rationale: \"branch\" is conceptually related to \"river\" because: Branch is like tree; Wood is like tree; Wood is like building; You are likely to find door in building; Door is like house; You are likely to find beam in house; You are likely to find beam in bridge; You are likely to find river in bridge. \"branch\" is conceptually related to \"landed\" because: Branch is like tree; Tree is like leaves; Trees has leaves; Forest is like trees; Squirrel is like forest; Squirrel is like small; Size is like small; Size is like large; Continent is like large; You are likely to find country in continent; You are likely to find land in country; Landed is like land; The word landed is derived from the word landed.\n",
      "DONE Example 71 - Running Time: 252.2 sec (4.2 min)\n",
      "\n",
      "question: The teacher assigned homework to the students.\n",
      "answer: The students groaned.\n",
      "q_keywords: ['teacher', 'students', 'homework']\n",
      "c_keywords: ['students', 'groaned']\n",
      "Todo kw_pairs: [('students', 'teacher'), ('students', 'homework')]\n",
      ">>> Rationale: \"students\" is conceptually related to \"teacher\" because: You are likely to find students in classroom; You are likely to find teacher in classroom. \"students\" is conceptually related to \"homework\" because: School is like students; You are likely to find homework in school.\n",
      "DONE Example 72 - Running Time: 4.1 sec (0.1 min)\n",
      "\n",
      "question: The seasons changed from summer to autumn.\n",
      "answer: Leaves fell from the trees.\n",
      "q_keywords: ['changed', 'seasons', 'summer']\n",
      "c_keywords: ['leaves', 'fell', 'trees']\n",
      "Todo kw_pairs: [('leaves', 'changed'), ('leaves', 'seasons')]\n",
      ">>> Rationale: \"leaves\" is conceptually related to \"seasons\" because: Tree is like leaves; Tree is like plant; Garden is like plant; Garden is like house; Home is like house; Home is like place; Camp is like place; Camp is like summer; Summer is like season; Seasons is a form of the word season.\n",
      "DONE Example 73 - Running Time: 209.3 sec (3.5 min)\n",
      "\n",
      "question: The politician was convicted of fraud.\n",
      "answer: He was removed from office.\n",
      "q_keywords: ['convicted', 'politician', 'fraud']\n",
      "c_keywords: ['office', 'removed']\n",
      "Todo kw_pairs: [('office', 'convicted'), ('office', 'politician')]\n",
      ">>> Rationale: \"office\" is conceptually related to \"convicted\" because: You are likely to find chair in office; You are likely to find chair in desk; Desk is like work; Computer is used for work; You are likely to find computer in table; You are likely to find floor in table; Floor is like house; Home is like house; You are likely to find home in city; London is a type of city; Knowledge is like london; Self convicted is like knowledge; Self convicted is like convicted.\n",
      "DONE Example 74 - Running Time: 168.4 sec (2.8 min)\n",
      "\n",
      "question: I pushed the wagon.\n",
      "answer: The wagon wheels spun forward.\n",
      "q_keywords: ['wagon', 'pushed']\n",
      "c_keywords: ['wagon', 'wheels', 'forward']\n",
      "Todo kw_pairs: [('wagon', 'pushed'), ('wheels', 'wagon')]\n",
      ">>> Rationale: \"wagon\" is conceptually related to \"pushed\" because: Wagon has wheels; Wheels is like muscle; You are likely to find muscle in arm; Hand is like arm; Hand is like body; Body is like human; Person is like human; Person does not want pushed. \"wheels\" is conceptually related to \"wagon\" because: Car is like wheels; Car is like automobile; You are likely to find automobile in garage; You are likely to find wagon in garage.\n",
      "DONE Example 75 - Running Time: 73.1 sec (1.2 min)\n",
      "\n",
      "question: The lobbyist persuaded the legislature to support the bill.\n",
      "answer: The legislature passed the bill.\n",
      "q_keywords: ['support', 'legislature', 'persuaded']\n",
      "c_keywords: ['legislature', 'passed']\n",
      "Todo kw_pairs: [('legislature', 'support'), ('legislature', 'persuaded')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 76 - Running Time: 93.0 sec (1.5 min)\n",
      "\n",
      "question: My closet was messy.\n",
      "answer: I organized it.\n",
      "q_keywords: ['messy', 'closet']\n",
      "c_keywords: ['organized']\n",
      "Todo kw_pairs: [('organized', 'messy'), ('organized', 'closet')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 77 - Running Time: 81.7 sec (1.4 min)\n",
      "\n",
      "question: I stayed up late.\n",
      "answer: I was tired in the morning.\n",
      "q_keywords: ['late', 'stayed']\n",
      "c_keywords: ['tired', 'morning']\n",
      "Todo kw_pairs: [('tired', 'late'), ('tired', 'stayed')]\n",
      ">>> Rationale: \"tired\" is conceptually related to \"late\" because: You would sleep because you want to tired; Sleep is like night; Evening is like night; Evening is like late.\n",
      "DONE Example 78 - Running Time: 63.8 sec (1.1 min)\n",
      "\n",
      "question: The man's pocket jingled as he walked.\n",
      "answer: His pocket was filled with coins.\n",
      "q_keywords: ['jingled', 'man', 'pocket']\n",
      "c_keywords: ['pocket', 'filled', 'coins']\n",
      "Todo kw_pairs: [('pocket', 'jingled'), ('pocket', 'man')]\n",
      ">>> Rationale: \"pocket\" is conceptually related to \"man\" because: You are likely to find hand in pocket; Hand is like body; Body is like person; Boy is like person; Boy is like man.\n",
      "DONE Example 79 - Running Time: 166.4 sec (2.8 min)\n",
      "\n",
      "question: Everyone in the class turned to stare at the student.\n",
      "answer: The student's phone rang.\n",
      "q_keywords: ['student', 'stare', 'turned']\n",
      "c_keywords: ['student', 'rang', 'phone']\n",
      "Todo kw_pairs: [('student', 'stare'), ('student', 'turned')]\n",
      ">>> Rationale: \"student\" is conceptually related to \"turned\" because: Student can study; In order for pass course to happen, study needs to happen; Something you do when you pass course is learn; Something you do when you reading is learn; Reading is like books; You are likely to find books in cabinet; You are likely to find screws in cabinet; Screws can be turned.\n",
      "DONE Example 80 - Running Time: 121.8 sec (2.0 min)\n",
      "\n",
      "question: The horse bucked.\n",
      "answer: A fly bit the horse.\n",
      "q_keywords: ['horse', 'bucked']\n",
      "c_keywords: ['horse', 'bit', 'fly']\n",
      "Todo kw_pairs: [('horse', 'bucked'), ('bit', 'horse')]\n",
      ">>> Rationale: \"horse\" is conceptually related to \"bucked\" because: Horse has four legs; Deer has four legs; Deer is a type of deer; Deer and deer have similar meanings; Buck is a type of deer; Bucked is a form of the word buck. \"bit\" is conceptually related to \"horse\" because: Bit is like small; Squirrel is like small; Squirrel is like animal; Horse is like animal.\n",
      "DONE Example 81 - Running Time: 89.9 sec (1.5 min)\n",
      "\n",
      "question: The jewelry thieves were caught.\n",
      "answer: The stolen jewelry was returned to its owners.\n",
      "q_keywords: ['caught', 'thieves', 'jewelry']\n",
      "c_keywords: ['stolen', 'jewelry', 'owners']\n",
      "Todo kw_pairs: [('stolen', 'caught'), ('stolen', 'thieves')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 82 - Running Time: 8.7 sec (0.1 min)\n",
      "\n",
      "question: Political violence broke out in the nation.\n",
      "answer: Many citizens took refuge in other territories.\n",
      "q_keywords: ['nation', 'broke', 'political']\n",
      "c_keywords: ['citizens', 'territories', 'took']\n",
      "Todo kw_pairs: [('citizens', 'nation'), ('citizens', 'broke')]\n",
      ">>> Rationale: \"citizens\" is conceptually related to \"nation\" because: You are likely to find citizens in country; Nation is like country. \"citizens\" is conceptually related to \"broke\" because: Citizens is a form of the word citizen; You are likely to find citizen in country; Nation is like country; Nation is like state; You are likely to find town in state; You are likely to find school in town; You are likely to find human in school; Person is like human; Person does not want broke.\n",
      "DONE Example 83 - Running Time: 94.0 sec (1.6 min)\n",
      "\n",
      "question: The woman was arrested.\n",
      "answer: She committed assault.\n",
      "q_keywords: ['woman', 'arrested']\n",
      "c_keywords: ['assault', 'committed']\n",
      "Todo kw_pairs: [('assault', 'woman'), ('assault', 'arrested')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 84 - Running Time: 149.7 sec (2.5 min)\n",
      "\n",
      "question: The woman read the newspaper.\n",
      "answer: She discovered the outcome of the election.\n",
      "q_keywords: ['woman', 'newspaper', 'read']\n",
      "c_keywords: ['election', 'outcome', 'discovered']\n",
      "Todo kw_pairs: [('election', 'woman'), ('election', 'newspaper')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 85 - Running Time: 155.6 sec (2.6 min)\n",
      "\n",
      "question: The sick child coughed on his friend.\n",
      "answer: His friend got sick.\n",
      "q_keywords: ['sick', 'friend', 'coughed']\n",
      "c_keywords: ['sick', 'friend', 'got']\n",
      "Todo kw_pairs: [('sick', 'friend'), ('sick', 'coughed')]\n",
      ">>> Rationale: \"sick\" is conceptually related to \"friend\" because: Something you do when you sick is cry; Baby can cry; Baby is like small; Flea is like small; Flea is like dog; Dog is like friend.\n",
      "DONE Example 86 - Running Time: 77.8 sec (1.3 min)\n",
      "\n",
      "question: The couple got engaged.\n",
      "answer: They planned a wedding.\n",
      "q_keywords: ['got', 'couple', 'engaged']\n",
      "c_keywords: ['wedding', 'planned']\n",
      "Todo kw_pairs: [('wedding', 'got'), ('wedding', 'couple')]\n",
      ">>> Rationale: \"wedding\" is conceptually related to \"got\" because: Ring is like wedding; Ring is like jewelry; Person wants jewelry; Person is like human; Baby is like human; Chick is like baby; Chick is like slang; Got is a word used in the context of slang. \"wedding\" is conceptually related to \"couple\" because: Marry is like wedding; Marry is like wed; Tie knot is like wed; Couple can tie knot.\n",
      "DONE Example 87 - Running Time: 230.1 sec (3.8 min)\n",
      "\n",
      "question: The woman contacted the real estate agent.\n",
      "answer: The woman planned to buy a condo.\n",
      "q_keywords: ['woman', 'contacted', 'estate']\n",
      "c_keywords: ['woman', 'planned', 'buy']\n",
      "Todo kw_pairs: [('woman', 'contacted'), ('woman', 'estate')]\n",
      ">>> Rationale: \"woman\" is conceptually related to \"contacted\" because: Lady is like woman; Lady is like female; Girl is like female; Chick is like girl; Chick is like young; Boy is like young; Boy is like child; Child is like human; Body is like human; Hand is like body; Touch is like hand; Touch is like contact; Uncontacted is like contact; The word uncontacted is derived from the word contacted. \"woman\" is conceptually related to \"estate\" because: Lady is like woman; Lady is like female; Girl is like female; Daughter is like girl; Daughter is like child; Child is like human; Baby is like human; Baby is like small; Hill is like small; Hill is like land; Property is like land; Property is like estate.\n",
      "DONE Example 88 - Running Time: 119.5 sec (2.0 min)\n",
      "\n",
      "question: The man won the lottery.\n",
      "answer: He became rich.\n",
      "q_keywords: ['man', 'won', 'lottery']\n",
      "c_keywords: ['rich']\n",
      "Todo kw_pairs: [('rich', 'man'), ('rich', 'won')]\n",
      ">>> Rationale: \"rich\" is conceptually related to \"man\" because: Person does not want rich; Man is like person. \"rich\" is conceptually related to \"won\" because: Person does not want rich; Person is like human; People is human; Party is like people; Party is like event; Rubber match is like event; Rubber match is like won.\n",
      "DONE Example 89 - Running Time: 82.4 sec (1.4 min)\n",
      "\n",
      "question: I lit the candle.\n",
      "answer: Wax dripped off the candle.\n",
      "q_keywords: ['candle', 'lit']\n",
      "c_keywords: ['candle', 'wax', 'dripped']\n",
      "Todo kw_pairs: [('candle', 'lit'), ('wax', 'candle')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 90 - Running Time: 40.6 sec (0.7 min)\n",
      "\n",
      "question: I spent the day at the pool.\n",
      "answer: My face got sunburned.\n",
      "q_keywords: ['day', 'pool', 'spent']\n",
      "c_keywords: ['sunburned', 'face', 'got']\n",
      "Todo kw_pairs: [('sunburned', 'day'), ('sunburned', 'pool')]\n",
      ">>> Rationale: \"sunburned\" is conceptually related to \"day\" because: Sunburned is like sunburn; Person does not want sunburn; Person wants time; Morning is like time; Morning is like day. \"sunburned\" is conceptually related to \"pool\" because: Sunburned is like sunburn; Person does not want sunburn; Body is like person; Sea is like body; Lake is like sea; You are likely to find fish in lake; You are likely to find fish in water; You are likely to find water in pool.\n",
      "DONE Example 91 - Running Time: 120.3 sec (2.0 min)\n",
      "\n",
      "question: The man received a parking ticket.\n",
      "answer: The parking meter expired.\n",
      "q_keywords: ['ticket', 'man', 'received']\n",
      "c_keywords: ['parking', 'expired', 'meter']\n",
      "Todo kw_pairs: [('parking', 'ticket'), ('parking', 'man')]\n",
      ">>> Rationale: \"parking\" is conceptually related to \"ticket\" because: Driveway is used for parking; You are likely to find driveway in yard; Yard is used for playing; Playing is used for having fun; Going to film is used for having fun; In order for going to film to happen, ticket needs to happen. \"parking\" is conceptually related to \"man\" because: Lot is like parking; Crowd is like lot; Crowd is like bunch; Bunch is like group; Family is like group; Daughter is like family; Daughter is like girl; Girl is like young; Child is like young; Boy is like child; Boy is like man.\n",
      "DONE Example 92 - Running Time: 182.0 sec (3.0 min)\n",
      "\n",
      "question: The woman became famous.\n",
      "answer: Photographers followed her.\n",
      "q_keywords: ['famous', 'woman']\n",
      "c_keywords: ['photographers', 'followed']\n",
      "Todo kw_pairs: [('photographers', 'famous'), ('photographers', 'woman')]\n",
      ">>> Rationale: \"photographers\" is conceptually related to \"woman\" because: You are likely to find photographers in waterfall; You are likely to find water in waterfall; You are likely to find boat in water; Boat is like small; Baby is like small; Baby is like human; Boy is like human; Boy is like young; Chick is like young; Chick is like girl; Girl is like female; Dress is like female; Dress is like woman.\n",
      "DONE Example 93 - Running Time: 62.5 sec (1.0 min)\n",
      "\n",
      "question: The girl wanted to wear earrings.\n",
      "answer: She got her ears pierced.\n",
      "q_keywords: ['wear', 'wanted', 'girl']\n",
      "c_keywords: ['pierced', 'ears', 'got']\n",
      "Todo kw_pairs: [('pierced', 'wear'), ('pierced', 'wanted')]\n",
      ">>> Rationale: \"pierced\" is conceptually related to \"wear\" because: Pertuse is like pierced; Pertuse is like punched; Person does not want punched; Girl is like person; Girl is like woman; Woman is like female; Dress is like female; Dress is like wear.\n",
      "DONE Example 94 - Running Time: 121.3 sec (2.0 min)\n",
      "\n",
      "question: My ears were ringing.\n",
      "answer: I went to a concert.\n",
      "q_keywords: ['ringing', 'ears']\n",
      "c_keywords: ['concert', 'went']\n",
      "Todo kw_pairs: [('concert', 'ringing'), ('concert', 'ears')]\n",
      ">>> Rationale: \"concert\" is conceptually related to \"ringing\" because: You are likely to find piano in concert; Piano is a type of instrument; Bell is like instrument; Bell is like ringing. \"concert\" is conceptually related to \"ears\" because: You are likely to find piano in concert; Piano is used for music; Music is like listening; In order for hearing news to happen, listening needs to happen; Something you do when you listening to radio is hearing news; In order for listening to radio to happen, ears needs to happen.\n",
      "DONE Example 95 - Running Time: 23.6 sec (0.4 min)\n",
      "\n",
      "question: I tidied up my house.\n",
      "answer: I was expecting company.\n",
      "q_keywords: ['house', 'tidied']\n",
      "c_keywords: ['company', 'expecting']\n",
      "Todo kw_pairs: [('company', 'house'), ('company', 'tidied')]\n",
      ">>> Rationale: \"company\" is conceptually related to \"house\" because: Company is like business; Bank is like business; Bank is like building; You are likely to find door in building; Door is like room; Room is like house.\n",
      "DONE Example 96 - Running Time: 110.7 sec (1.8 min)\n",
      "\n",
      "question: The airline mishandled my luggage.\n",
      "answer: They offered me compensation.\n",
      "q_keywords: ['mishandled', 'airline', 'luggage']\n",
      "c_keywords: ['compensation', 'offered']\n",
      "Todo kw_pairs: [('compensation', 'mishandled'), ('compensation', 'airline')]\n",
      ">>> No rationale (path).\n",
      "DONE Example 97 - Running Time: 20.1 sec (0.3 min)\n",
      "\n",
      "question: The computer was expensive to fix.\n",
      "answer: I bought a new one.\n",
      "q_keywords: ['fix', 'computer', 'expensive']\n",
      "c_keywords: ['bought', 'new']\n",
      "Todo kw_pairs: [('bought', 'fix'), ('bought', 'computer')]\n",
      ">>> Rationale: \"bought\" is conceptually related to \"computer\" because: Bought is a form of the word buy; Buy is like money; You would work because you want to money; Computer is used for work.\n",
      "DONE Example 98 - Running Time: 65.0 sec (1.1 min)\n",
      "\n",
      "question: The woman was in a bad mood.\n",
      "answer: She told her friend to leave her alone.\n",
      "q_keywords: ['woman', 'bad', 'mood']\n",
      "c_keywords: ['told', 'leave', 'friend']\n",
      "Todo kw_pairs: [('told', 'woman'), ('told', 'bad')]\n",
      ">>> Rationale: \"told\" is conceptually related to \"woman\" because: Telt and told have similar meanings; Telt is a form of the word tell; In order for forgive to happen, tell needs to happen; Something you do when you forgive is sex; Baby is created by sex; Baby is like human; Child is like human; Boy is like child; Boy is like young; Chick is like young; Chick is like girl; Girl is like female; Dress is like female; Dress is like woman.\n",
      "DONE Example 99 - Running Time: 102.2 sec (1.7 min)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of val_set is {len(val_set)}\")\n",
    "\n",
    "val_set_kb = val_set.copy()\n",
    "\n",
    "for e_id, ex in enumerate(tqdm.notebook.tqdm(val_set_kb)):\n",
    "    # cur_rationale = retrieve_knowledge(ex, e_id, pair_cnt=1, verbose=True)\n",
    "    cur_rationale = retrieve_knowledge(ex, e_id, pair_cnt=2, verbose=True)\n",
    "    ex[\"rationale\"] = cur_rationale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If `pair_cnt=1`, run time: about 1h 30m\n",
    "* If `pair_cnt=2`, run time: about 3h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDJZ98qw7JFo"
   },
   "source": [
    "Now, we need to change the prompt creation functions to include this rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "3wtsAXTL7IZ2"
   },
   "outputs": [],
   "source": [
    "def single_example_prompt_with_kb(example, include_answer=False):\n",
    "    prompt = f\"Q: {example['premise']} {question_type_to_nl[example['question']]}\" + \\\n",
    "             f\"\\n1) {example['choice1']}\\n2) {example['choice2']}\" + \\\n",
    "             f\"\\nRationale: {example['rationale']}\\n\"\n",
    "\n",
    "    if include_answer:\n",
    "      prompt += f\"\\nA: {example['label'] + 1}\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkGOatUz8P5M"
   },
   "source": [
    "Let's look at an example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LqJJOJt08PJC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The woman felt lonely. What might have happened as a result?\n",
      "1) She renovated her kitchen.\n",
      "2) She adopted a cat.\n",
      "Rationale: \"woman\" is conceptually related to \"adopt\" because: Lady is like woman; Lady is like female; Girl is like female; Chick is like girl; Chick is like baby; Baby is like child; Adopt is like child.\n",
      "\n",
      "A: 2\n",
      "\n",
      "Q: The mother needed help looking after her children. What might have happened as a result?\n",
      "1) She sent the children to daycare.\n",
      "2) She gave up custody of the children.\n",
      "Rationale: \"mother\" is conceptually related to \"children\" because: Daughter is like mother; Daughter is like child; You are likely to find child in school; You are likely to find children in school.\n",
      "\n",
      "A: 1\n",
      "\n",
      "Q: I learned how to play the board game. What could have caused this?\n",
      "1) My friend explained the rules to me.\n",
      "2) My friend got the rules wrong.\n",
      "Rationale: \"learn\" is conceptually related to \"game\" because: Something you do when you reading is learn; Sometimes reading causes learning; Playing is used for learning; Toy is like playing; Toy is like fun; Play is like fun; Game is like play. \"game\" is conceptually related to \"rules\" because: Game is like play; Play is like fun; Toy is like fun; Toy is like playing; Card is like playing; Card is like king; Rule is like king; Rule is like law; Law is like rules.\n",
      "\n",
      "A: 1\n",
      "\n",
      "Q: The woman's eyeglasses fogged up. What could have caused this?\n",
      "1) She reclined by the pool.\n",
      "2) She entered the sauna.\n",
      "Rationale: \"eyeglasses\" is conceptually related to \"fog\" because: Pair of glasses and eyeglasses have similar meanings; Pair of glasses is like lens; Lens is part of eye; Eye is like organ; You are likely to find organ in body; Hand is like body; You are likely to find cigarette in hand; Smoke is like cigarette; Smoke is like fog. \"fog\" is conceptually related to \"sauna\" because: Mist is like fog; Mist is like water; Steam is like water; Steam is like air; Air is a type of gas; Oxygen is a type of gas; You are likely to find oxygen in space shuttle; You are likely to find human in space shuttle; You are likely to find human in sauna.\n",
      "\n",
      "A: 2\n",
      "\n",
      "Q: I ran out of breath. What could have caused this?\n",
      "1) I climbed several flights of stairs.\n",
      "2) I read several chapters of the book.\n",
      "Rationale: \"breath\" is conceptually related to \"climb\" because: Breath is like air; Sky is like air; Sky is like up; Climb is like up. \"climb\" is conceptually related to \"stair\" because: Stairs up is used for climb; You are likely to find stairs up in building; You are likely to find staircase in building; The word staircase is derived from the word stair.\n",
      "\n",
      "A: 1\n",
      "\n",
      "Q: The man turned on the faucet. What might have happened as a result?\n",
      "1) The toilet filled with water.\n",
      "2) Water flowed from the spout.\n",
      "Rationale: \"flowed\" is conceptually related to \"man\" because: Backwash is like flowed; Backwash is like food; Farmer is like food; Farmer is like man. \"flowed\" is conceptually related to \"turned\" because: Backwash is like flowed; Backwash is like backward; Backward is similar to backward; Transposed is similar to backward; Reversed and transposed have similar meanings; Turned is similar to reversed; Turned and turned have similar meanings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = create_prompt(in_context_examples_kb, val_set_kb[0], \n",
    "                       single_fn=single_example_prompt_with_kb)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEJrRJHI8gA2"
   },
   "source": [
    "Finally, let's predict the answers, compute the accuracy, and save the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891d2ff9411949199202d229529e68f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_predictions_kb = [predict_retry(\n",
    "    generator, create_prompt(\n",
    "        in_context_examples_kb, ex, single_fn=single_example_prompt_with_kb), \n",
    "        gen_config=gen_config)\n",
    "    for ex in tqdm.notebook.tqdm(val_set_kb)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuray is 0.47\n"
     ]
    }
   ],
   "source": [
    "predicts = val_predictions_kb\n",
    "incorrect_predicts = []\n",
    "for idx in range(len(predicts)):\n",
    "    predict = predicts[idx]\n",
    "    gold = val_set_kb[idx]\n",
    "    if gold[\"label\"] != predict:\n",
    "        incorrect_predicts.append(predict)\n",
    "\n",
    "accuracy = 1 - len(incorrect_predicts) / len(predicts)\n",
    "print(f\"The prediction accuray is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to output/kb_predictions-gpt-neo-125m.jsonl\n"
     ]
    }
   ],
   "source": [
    "res_fp_3 = \"output/kb_predictions-{}.jsonl\".format(MODEL_NAME.split(\"/\")[-1])\n",
    "print(f\"Saving to {res_fp_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(res_fp_3, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for ex, pred in zip(val_set_kb, val_predictions_kb):\n",
    "        new_ex = ex.copy()\n",
    "        new_ex[\"prediction\"] = pred\n",
    "        f_out.write(json.dumps(new_ex) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuray is 0.47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'premise': 'I tidied up my house.',\n",
       "  'choice1': 'I was swamped with work.',\n",
       "  'choice2': 'I was expecting company.',\n",
       "  'question': 'cause',\n",
       "  'idx': 96,\n",
       "  'label': 1,\n",
       "  'rationale': '\"company\" is conceptually related to \"house\" because: Company is like business; Bank is like business; Bank is like building; You are likely to find door in building; Door is like room; Room is like house.',\n",
       "  'prediction': 0},\n",
       " {'premise': 'The man perceived that the woman looked different.',\n",
       "  'choice1': 'The woman got her hair cut.',\n",
       "  'choice2': 'The woman wore a bracelet.',\n",
       "  'question': 'cause',\n",
       "  'idx': 65,\n",
       "  'label': 0,\n",
       "  'rationale': '\"woman\" is conceptually related to \"looked\" because: Dress is like woman; Dress is like female; Girl is like female; Chick is like girl; Chick is like young; Boy is like young; Boy is like child; Child is like human; Baby is like human; Baby is like small; Drop is like small; Drop is like down; The word downlooked is derived from the word down; The word downlooked is derived from the word looked. \"woman\" is conceptually related to \"perceived\" because: Dress is like woman; Dress is like female; Girl is like female; Chick is like girl; Chick is like young; Boy is like young; Boy is like child; Child is like human; Baby is like human; Baby is like small; Boat is like small; Boat is like vehicle; Thinking distance is like vehicle; Thinking distance is like perceived.',\n",
       "  'prediction': 1},\n",
       " {'premise': 'The man turned on the faucet.',\n",
       "  'choice1': 'The toilet filled with water.',\n",
       "  'choice2': 'Water flowed from the spout.',\n",
       "  'question': 'effect',\n",
       "  'idx': 0,\n",
       "  'label': 1,\n",
       "  'rationale': '\"flowed\" is conceptually related to \"man\" because: Backwash is like flowed; Backwash is like food; Farmer is like food; Farmer is like man. \"flowed\" is conceptually related to \"turned\" because: Backwash is like flowed; Backwash is like backward; Backward is similar to backward; Transposed is similar to backward; Reversed and transposed have similar meanings; Turned is similar to reversed; Turned and turned have similar meanings.',\n",
       "  'prediction': 0},\n",
       " {'premise': 'The bowling ball knocked over the bowling pins.',\n",
       "  'choice1': 'The man rolled the bowling ball down the alley.',\n",
       "  'choice2': 'The man dropped the bowling ball on his foot.',\n",
       "  'question': 'cause',\n",
       "  'idx': 10,\n",
       "  'label': 0,\n",
       "  'rationale': '\"bowling\" is conceptually related to \"ball\" because: Bowling is a type of sport; Baseball is a type of sport; Baseball is used for playing game; Ball is used for playing game. \"bowling\" is conceptually related to \"knocked\" because: Bowling is a type of sport; Soccer is a type of sport; Knockdown is a word used in the context of soccer; Knockdown is like knocked.',\n",
       "  'prediction': 1},\n",
       " {'premise': 'The man won the lottery.',\n",
       "  'choice1': 'He became rich.',\n",
       "  'choice2': 'He owed money.',\n",
       "  'question': 'effect',\n",
       "  'idx': 89,\n",
       "  'label': 0,\n",
       "  'rationale': '\"rich\" is conceptually related to \"man\" because: Person does not want rich; Man is like person. \"rich\" is conceptually related to \"won\" because: Person does not want rich; Person is like human; People is human; Party is like people; Party is like event; Rubber match is like event; Rubber match is like won.',\n",
       "  'prediction': 1},\n",
       " {'premise': 'My skin broke out into a rash.',\n",
       "  'choice1': 'I brushed against poison ivy in my yard.',\n",
       "  'choice2': 'I eradicated the poison ivy from my yard.',\n",
       "  'question': 'cause',\n",
       "  'idx': 50,\n",
       "  'label': 0,\n",
       "  'rationale': '\"poison\" is conceptually related to \"skin\" because: Poison is like liquid; Blood is like liquid; Blood is like body; Body is like skin. \"poison\" is conceptually related to \"broke\" because: Poison is like liquid; Drink is like liquid; Drink is like glass; Glass can be broken; Broke is like broken.',\n",
       "  'prediction': 1},\n",
       " {'premise': 'The couple got engaged.',\n",
       "  'choice1': 'They planned a wedding.',\n",
       "  'choice2': 'They took some time apart.',\n",
       "  'question': 'effect',\n",
       "  'idx': 87,\n",
       "  'label': 0,\n",
       "  'rationale': '\"wedding\" is conceptually related to \"got\" because: Ring is like wedding; Ring is like jewelry; Person wants jewelry; Person is like human; Baby is like human; Chick is like baby; Chick is like slang; Got is a word used in the context of slang. \"wedding\" is conceptually related to \"couple\" because: Marry is like wedding; Marry is like wed; Tie knot is like wed; Couple can tie knot.',\n",
       "  'prediction': 1},\n",
       " {'premise': 'I stayed up late.',\n",
       "  'choice1': 'I had vivid dreams that night.',\n",
       "  'choice2': 'I was tired in the morning.',\n",
       "  'question': 'effect',\n",
       "  'idx': 78,\n",
       "  'label': 1,\n",
       "  'rationale': '\"tired\" is conceptually related to \"late\" because: You would sleep because you want to tired; Sleep is like night; Evening is like night; Evening is like late.',\n",
       "  'prediction': 0},\n",
       " {'premise': 'The politician was convicted of fraud.',\n",
       "  'choice1': 'He campaigned for re-election.',\n",
       "  'choice2': 'He was removed from office.',\n",
       "  'question': 'effect',\n",
       "  'idx': 74,\n",
       "  'label': 1,\n",
       "  'rationale': '\"office\" is conceptually related to \"convicted\" because: You are likely to find chair in office; You are likely to find chair in desk; Desk is like work; Computer is used for work; You are likely to find computer in table; You are likely to find floor in table; Floor is like house; Home is like house; You are likely to find home in city; London is a type of city; Knowledge is like london; Self convicted is like knowledge; Self convicted is like convicted.',\n",
       "  'prediction': 0},\n",
       " {'premise': 'My ears were ringing.',\n",
       "  'choice1': 'I went to a museum.',\n",
       "  'choice2': 'I went to a concert.',\n",
       "  'question': 'cause',\n",
       "  'idx': 95,\n",
       "  'label': 1,\n",
       "  'rationale': '\"concert\" is conceptually related to \"ringing\" because: You are likely to find piano in concert; Piano is a type of instrument; Bell is like instrument; Bell is like ringing. \"concert\" is conceptually related to \"ears\" because: You are likely to find piano in concert; Piano is used for music; Music is like listening; In order for hearing news to happen, listening needs to happen; Something you do when you listening to radio is hearing news; In order for listening to radio to happen, ears needs to happen.',\n",
       "  'prediction': 0},\n",
       " {'premise': 'The seasons changed from summer to autumn.',\n",
       "  'choice1': 'People evacuated their homes.',\n",
       "  'choice2': 'Leaves fell from the trees.',\n",
       "  'question': 'effect',\n",
       "  'idx': 73,\n",
       "  'label': 1,\n",
       "  'rationale': '\"leaves\" is conceptually related to \"seasons\" because: Tree is like leaves; Tree is like plant; Garden is like plant; Garden is like house; Home is like house; Home is like place; Camp is like place; Camp is like summer; Summer is like season; Seasons is a form of the word season.',\n",
       "  'prediction': 0},\n",
       " {'premise': 'The woman lavished her friend with flattery.',\n",
       "  'choice1': 'She wanted to ask her friend for a favor.',\n",
       "  'choice2': \"She was irritated with her friend's whining.\",\n",
       "  'question': 'cause',\n",
       "  'idx': 56,\n",
       "  'label': 0,\n",
       "  'rationale': '\"friend\" is conceptually related to \"woman\" because: Friend is like person; Person is like woman.',\n",
       "  'prediction': 1},\n",
       " {'premise': \"The journalist wrote a biography about the humanitarian's life.\",\n",
       "  'choice1': 'The humanitarian was difficult for the journalist to interview.',\n",
       "  'choice2': \"The journalist was intrigued by the humanitarian's work.\",\n",
       "  'question': 'cause',\n",
       "  'idx': 44,\n",
       "  'label': 1,\n",
       "  'rationale': 'None.',\n",
       "  'prediction': 0},\n",
       " {'premise': 'The girl refused to eat her vegetables.',\n",
       "  'choice1': 'Her father told her to drink her milk.',\n",
       "  'choice2': 'Her father took away her dessert.',\n",
       "  'question': 'effect',\n",
       "  'idx': 28,\n",
       "  'label': 1,\n",
       "  'rationale': '\"dessert\" is conceptually related to \"eat\" because: Cake is like dessert; Cake is like round; Plate is like round; Plate is like food; You would sate hunger because you want to food; In order for sate hunger to happen, find food needs to happen; In order for diminish own hunger to happen, find food needs to happen; In order for diminish own hunger to happen, eat needs to happen.',\n",
       "  'prediction': 0},\n",
       " {'premise': 'I twisted the cap off the soda bottle.',\n",
       "  'choice1': 'The soda fizzed.',\n",
       "  'choice2': 'The soda leaked out.',\n",
       "  'question': 'effect',\n",
       "  'idx': 41,\n",
       "  'label': 0,\n",
       "  'rationale': '\"soda\" is conceptually related to \"twisted\" because: Coke is a type of soda; Coke is a type of beverage; Wine is a type of beverage; Wine is like red; Blood is like red; Wound is like blood; Twisted is like wound.',\n",
       "  'prediction': 1},\n",
       " {'premise': 'My eyes became red and puffy.',\n",
       "  'choice1': 'I was sobbing.',\n",
       "  'choice2': 'I was laughing.',\n",
       "  'question': 'cause',\n",
       "  'idx': 7,\n",
       "  'label': 0,\n",
       "  'rationale': '\"sobbing\" is conceptually related to \"puffy\" because: Cry is like sobbing; Cry is like tears; Tears is like tear; Drop is like tear; Drop is like water; Rain is a type of water; Rain is like sky; Cloud is like sky; Cloud is like puffy. \"sobbing\" is conceptually related to \"red\" because: Cry is like sobbing; Cry is like eyes; Eyes is blue; Blue is like color; Color is like red.',\n",
       "  'prediction': 1},\n",
       " {'premise': 'The driver got a flat tire.',\n",
       "  'choice1': 'He went over the speed limit.',\n",
       "  'choice2': 'He ran over a nail.',\n",
       "  'question': 'cause',\n",
       "  'idx': 25,\n",
       "  'label': 1,\n",
       "  'rationale': '\"ran\" is conceptually related to \"tire\" because: Ran is a form of the word run; Run is like walk; Walk is like moving; Drive is like moving; Drive is like car; Tire is part of car.',\n",
       "  'prediction': 0},\n",
       " {'premise': \"The driver turned on the car's headlights.\",\n",
       "  'choice1': 'He heard thunder.',\n",
       "  'choice2': 'The sun went down.',\n",
       "  'question': 'cause',\n",
       "  'idx': 27,\n",
       "  'label': 1,\n",
       "  'rationale': '\"went\" is conceptually related to \"car\" because: Yode is like went; Yode is a form of the word go; It cannot be both stop and go; Stop is like red; Car is red.',\n",
       "  'prediction': 0},\n",
       " {'premise': 'The woman became famous.',\n",
       "  'choice1': 'Photographers followed her.',\n",
       "  'choice2': 'Her family avoided her.',\n",
       "  'question': 'effect',\n",
       "  'idx': 93,\n",
       "  'label': 0,\n",
       "  'rationale': '\"photographers\" is conceptually related to \"woman\" because: You are likely to find photographers in waterfall; You are likely to find water in waterfall; You are likely to find boat in water; Boat is like small; Baby is like small; Baby is like human; Boy is like human; Boy is like young; Chick is like young; Chick is like girl; Girl is like female; Dress is like female; Dress is like woman.',\n",
       "  'prediction': 1},\n",
       " {'premise': 'The student misspelled the word.',\n",
       "  'choice1': 'The teacher corrected her.',\n",
       "  'choice2': 'The teacher dismissed her.',\n",
       "  'question': 'effect',\n",
       "  'idx': 20,\n",
       "  'label': 0,\n",
       "  'rationale': '\"teacher\" is conceptually related to \"student\" because: You are likely to find teacher in classroom; You are likely to find student in classroom. \"teacher\" is conceptually related to \"word\" because: You are likely to find teacher in classroom; You are likely to find paper in classroom; Letter is like paper; Letter is part of word.',\n",
       "  'prediction': 1}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts = []\n",
    "with open(res_fp_3, \"r\", encoding=\"utf-8\") as f_in:\n",
    "    for line in f_in:\n",
    "        predicts.append(json.loads(line))\n",
    "\n",
    "incorrect_predicts = []\n",
    "for predict in predicts:\n",
    "    if predict[\"label\"] != predict[\"prediction\"]:\n",
    "        incorrect_predicts.append(predict)\n",
    "\n",
    "accuracy = 1 - len(incorrect_predicts) / len(predicts)\n",
    "print(f\"The prediction accuray is {accuracy:.2f}\")\n",
    "\n",
    "selected_incorrect_predicts = random.sample(incorrect_predicts, 20)\n",
    "selected_incorrect_predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
